{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3faa7771-bb3a-4024-9c4b-9067bb2e0f17",
      "metadata": {
        "id": "3faa7771-bb3a-4024-9c4b-9067bb2e0f17"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Bo-Ni/ProteinMechanicsDiffusionDesign_pLDM/blob/main/notebook_for_colab/pLDM_inferring_standalong_colab.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Preparation:\n",
        "### 0-1. Add packages"
      ],
      "metadata": {
        "id": "xQvn0r7NYIgW"
      },
      "id": "xQvn0r7NYIgW"
    },
    {
      "cell_type": "code",
      "source": [
        "# !python --version"
      ],
      "metadata": {
        "id": "YvjMugf4BE0U"
      },
      "id": "YvjMugf4BE0U",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install python3.9"
      ],
      "metadata": {
        "id": "wwtMjKUBAkGO"
      },
      "id": "wwtMjKUBAkGO",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.pop(0)\n",
        "# sys.path.insert(0, '/usr/bin/python3.9')"
      ],
      "metadata": {
        "id": "0Og83AHiAz8h"
      },
      "id": "0Og83AHiAz8h",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys\n",
        "import math\n",
        "\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #turn off CUDA if needed\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "path_1 = '/opt/bin/'\n",
        "dssp_file = path_1+'mkdssp'\n",
        "\n",
        "file_exists = os.path.exists(dssp_file)\n",
        "if not (file_exists):\n",
        "  print('\\033[1;32m For the 1st run, ')\n",
        "  # ==============================================\n",
        "  print('a. Install omegafold...')\n",
        "  # install omegafold\n",
        "  # ref: https://github.com/HeliXonProtein/OmegaFold\n",
        "  !pip install git+https://github.com/HeliXonProtein/OmegaFold.git\n",
        "\n",
        "  # time-consuming step:\n",
        "  # Downloading weights from https://helixon.s3.amazonaws.com/release1.pt to /root/.cache/omegafold_ckpt/model.pt\n",
        "  !mkdir /root/.cache/omegafold_ckpt\n",
        "  !wget https://helixon.s3.amazonaws.com/release1.pt -O /root/.cache/omegafold_ckpt/model.pt\n",
        "\n",
        "  print('b. Install DSSP...')\n",
        "  # download an mkdssp\n",
        "  # ==============================================\n",
        "  # download things\n",
        "  print(os.popen(f\"wget https://www.dropbox.com/s/v4azy9z9yojg1c6/mkdssp -P {path_1}\").read())\n",
        "  #\n",
        "  !chmod u+x /opt/bin/mkdssp\n",
        "\n",
        "else:\n",
        "  print('This is not the first run... ')"
      ],
      "metadata": {
        "id": "cLDyOsEcYYsy",
        "outputId": "eaedc24e-0387-4e3b-c814-79b860f15b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cLDyOsEcYYsy",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32m For the 1st run, \n",
            "a. Install omegafold...\n",
            "Collecting git+https://github.com/HeliXonProtein/OmegaFold.git\n",
            "  Cloning https://github.com/HeliXonProtein/OmegaFold.git to /tmp/pip-req-build-pn2hpkxv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/HeliXonProtein/OmegaFold.git /tmp/pip-req-build-pn2hpkxv\n",
            "  Resolved https://github.com/HeliXonProtein/OmegaFold.git to commit 313c873ad190b64506a497c926649e15fcd88fcd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch@ https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (from OmegaFold==0.0.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m989.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython (from OmegaFold==0.0.0)\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython->OmegaFold==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch@ https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl->OmegaFold==0.0.0) (4.5.0)\n",
            "Building wheels for collected packages: OmegaFold\n",
            "  Building wheel for OmegaFold (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for OmegaFold: filename=OmegaFold-0.0.0-py3-none-any.whl size=55664 sha256=24df3f36e61ee878b35eb87573a55e295efbbf9d4d798c3909e3941f3354fb92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-offc34c6/wheels/fe/57/01/3ce12996dd37debe3ee6a02e8748fffdd4df7b885c5bb8071d\n",
            "Successfully built OmegaFold\n",
            "Installing collected packages: torch, biopython, OmegaFold\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OmegaFold-0.0.0 biopython-1.81 torch-1.12.0+cu113\n",
            "--2023-10-01 02:23:01--  https://helixon.s3.amazonaws.com/release1.pt\n",
            "Resolving helixon.s3.amazonaws.com (helixon.s3.amazonaws.com)... 52.216.208.57, 52.216.219.97, 3.5.28.234, ...\n",
            "Connecting to helixon.s3.amazonaws.com (helixon.s3.amazonaws.com)|52.216.208.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3181611124 (3.0G) [binary/octet-stream]\n",
            "Saving to: ‘/root/.cache/omegafold_ckpt/model.pt’\n",
            "\n",
            "/root/.cache/omegaf 100%[===================>]   2.96G  15.7MB/s    in 3m 9s   \n",
            "\n",
            "2023-10-01 02:26:11 (16.1 MB/s) - ‘/root/.cache/omegafold_ckpt/model.pt’ saved [3181611124/3181611124]\n",
            "\n",
            "b. Install DSSP...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add some packages\n",
        "try:\n",
        "  print('\\033[1;32m a. on Biopython...')\n",
        "  from Bio.PDB import PDBParser\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install biopython').read())\n",
        "\n",
        "try:\n",
        "  print('\\033[1;32m b. on kornia...')\n",
        "  import kornia.augmentation\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install kornia').read())\n",
        "\n",
        "try:\n",
        "  print('\\033[1;32m c. on einops...')\n",
        "  from einops import rearrange, repeat, reduce\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install einops').read())\n",
        "\n",
        "try:\n",
        "  from einops_exts import rearrange_many, repeat_many, check_shape\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install einops-exts').read())\n",
        "\n",
        "try:\n",
        "  import pytorch_warmup as warmup\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install pytorch-warmup').read())\n",
        "\n",
        "try:\n",
        "  from ema_pytorch import EMA\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install ema-pytorch').read())\n",
        "\n",
        "try:\n",
        "  from accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install accelerate').read())\n",
        "\n",
        "try:\n",
        "  import py3Dmol\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install py3Dmol').read())\n",
        "\n",
        "# added\n",
        "try:\n",
        "  import esm\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install fair-esm').read())\n",
        "\n",
        "try:\n",
        "  import torchinfo\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install torchinfo').read())"
      ],
      "metadata": {
        "id": "pUhFAfNfZFsW",
        "outputId": "752273a6-193b-4ccf-89c5-87a2667d5b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pUhFAfNfZFsW",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32m a. on Biopython...\n",
            "\u001b[1;32m b. on kornia...\n",
            "\u001b[1;32m c. on einops...\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2. copy src from github"
      ],
      "metadata": {
        "id": "R3JeHPw9bOMQ"
      },
      "id": "R3JeHPw9bOMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, os, sys, glob\n",
        "\n",
        "# just script, no install is used\n",
        "if not os.path.isdir(\"ProteinMPNN\"):\n",
        "  os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "# sys.path.append('/content/ProteinMPNN/')\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "if not os.path.isdir(\"ProteinMechanicsDiffusionDesign_pLDM\"):\n",
        "  os.system(\"git clone -q https://github.com/Bo-Ni/ProteinMechanicsDiffusionDesign_pLDM.git\")\n",
        "# sys.path.append('/content/ProteinMechanicsDiffusionDesign_pLDM/ProteinMechanicsDiffusionDesign/')\n",
        "sys.path.append('/content/ProteinMechanicsDiffusionDesign_pLDM/')"
      ],
      "metadata": {
        "id": "wV1H9Bf0bYat"
      },
      "id": "wV1H9Bf0bYat",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a slient test\n",
        "import ProteinMechanicsDiffusionDesign.UtilityPack as UtilityPack\n",
        "import ProteinMechanicsDiffusionDesign.DataSetPack as DataSetPack\n",
        "import ProteinMechanicsDiffusionDesign.ModelPack as ModelPack\n",
        "import ProteinMechanicsDiffusionDesign.TrainerPack as TrainerPack\n",
        "import ProteinMechanicsDiffusionDesign.PostMDPack as PostMDPack"
      ],
      "metadata": {
        "id": "TBNNv-Oo7Ftw"
      },
      "id": "TBNNv-Oo7Ftw",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3. Download the model files"
      ],
      "metadata": {
        "id": "BRkJe9AUNRfy"
      },
      "id": "BRkJe9AUNRfy"
    },
    {
      "cell_type": "code",
      "source": [
        "# just script, no install is used\n",
        "this_working_path = '/content/Trained_model/'\n",
        "\n",
        "if not os.path.isdir(this_working_path):\n",
        "  print('Creating working path...')\n",
        "  print(os.popen('mkdir '+this_working_path).read())\n",
        "  print('Done.')\n",
        "  print('Downing files...')\n",
        "\n",
        "this_file = this_working_path+'model_pack.pickle'\n",
        "file_exists = os.path.exists(this_file)\n",
        "if not (file_exists):\n",
        "  # download things\n",
        "  this_link='https://www.dropbox.com/scl/fi/i2sull7ftjwrrzeaxo8v1/model_pack.pickle?rlkey=7wy5zynrl6m8azufklq3fy8ql&dl=0'\n",
        "  cmd_line = f\"wget -O {this_file} {this_link}\"\n",
        "  print(os.popen(cmd_line).read())\n",
        "\n",
        "#\n",
        "this_file = this_working_path+'data_pack.pickle'\n",
        "file_exists = os.path.exists(this_file)\n",
        "if not (file_exists):\n",
        "  # download things\n",
        "  this_link='https://www.dropbox.com/scl/fi/z7sz0q2nsjn85kyh68p86/data_pack.pickle?rlkey=bwm9fgf29ze8o516r155zg4gl&dl=0'\n",
        "  cmd_line = f\"wget -O {this_file} {this_link}\"\n",
        "  print(os.popen(cmd_line).read())\n",
        "\n",
        "\n",
        "\n",
        "  # os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "# sys.path.append('/content/ProteinMPNN/')"
      ],
      "metadata": {
        "id": "h5l3MrGnNQuJ",
        "outputId": "ec1f2a2d-1dd2-4a20-a4f7-fb5d3ffdc189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h5l3MrGnNQuJ",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Working part"
      ],
      "metadata": {
        "id": "jc-QOhsnGAm9"
      },
      "id": "jc-QOhsnGAm9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0. Check the floor"
      ],
      "metadata": {
        "id": "frn-YZYnGN51"
      },
      "id": "frn-YZYnGN51"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys"
      ],
      "metadata": {
        "id": "AqtZKiwq_CaO"
      },
      "id": "AqtZKiwq_CaO",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Here is : \\n', os.popen('pwd').read())\n",
        "print('What we get in hardware: \\n', os.popen('nvidia-smi').read())"
      ],
      "metadata": {
        "id": "o6kXrcuX_LtK",
        "outputId": "9a270d58-1ee2-43c6-ca5a-56281717feb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o6kXrcuX_LtK",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is : \n",
            " /content\n",
            "\n",
            "What we get in hardware: \n",
            " Sun Oct  1 02:37:53 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    44W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"What we have in software: \\n Torch version:\", torch.__version__)\n",
        "print('Python: ', sys.version) # no switch case code"
      ],
      "metadata": {
        "id": "AXDZZWJk_ZUY",
        "outputId": "cade3a17-6744-4607-8e18-699b66462942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AXDZZWJk_ZUY",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What we have in software: \n",
            " Torch version: 1.12.0+cu113\n",
            "Python:  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"What we have in software: \\n Torch version:\", torch.__version__)\n",
        "print('Python: ', sys.version) # no switch case code"
      ],
      "metadata": {
        "id": "p_Ogx3y-_2Pr",
        "outputId": "cd054fc0-01d0-49ea-f7d8-1bd366e8d662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "p_Ogx3y-_2Pr",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What we have in software: \n",
            " Torch version: 1.12.0+cu113\n",
            "Python:  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('What hardware the software see:')\n",
        "device = torch.device(\n",
        "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "print(device)\n",
        "num_of_gpus = torch.cuda.device_count()\n",
        "print(\"# of GPU\", num_of_gpus)"
      ],
      "metadata": {
        "id": "BVzrkN8kGgbY",
        "outputId": "f944a00c-46a7-41d9-f1cd-abf5f31c422f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BVzrkN8kGgbY",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What hardware the software see:\n",
            "cuda:0\n",
            "# of GPU 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yAvkOLvbGgep"
      },
      "id": "yAvkOLvbGgep",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Setup the problem"
      ],
      "metadata": {
        "id": "gL68ZWN-Gt-G"
      },
      "id": "gL68ZWN-Gt-G"
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for package debugging\n",
        "# for debug\n",
        "import importlib\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SuRSZhjbGgi1"
      },
      "id": "SuRSZhjbGgi1",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import PD_pLMProbXDiff.UtilityPack as UtilityPack\n",
        "# # run this when updating the package\n",
        "# importlib.reload(UtilityPack)"
      ],
      "metadata": {
        "id": "XZ-n_YDYG518"
      },
      "id": "XZ-n_YDYG518",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Global control key setup\n",
        "# ===============================================\n",
        "# Control keys:\n",
        "# This one will be directly modified for each task\n",
        "CKeys = dict(\n",
        "    #\n",
        "    Running_Type=2, # 1-local:engaging cluster, 2-supercould cluster, 3-google colab, 4-local ubuntu\n",
        "    #\n",
        "    # Working_Mode=1, # 1-training, 2-sampling for test\n",
        "    Working_Mode=2,\n",
        "    #\n",
        "    # IF_FirstRun=1,  # 1-1st run of training; otherwise, # of training run\n",
        "    IF_FirstRun=2,  # 1-1st run of training; otherwise, # of training run\n",
        "    #\n",
        "    # 1-SecStr-ModelB, 2-MD-ModelB, 3-SecStr-ModelA, 4-MD-ModelA\n",
        "    # 5-SecStr-ModelB-Embdding, 6-MD-ModelB, 7-SecStr-ModelA-pLM, 8-MD-ModelA\n",
        "    # 9-MD-Predictor-ModelB, 10-\n",
        "    # 11-MD-ModelB\n",
        "    Problem_ID=11, # 8, # 6,\n",
        "    #\n",
        "    # Debug=1, # 1-debug mode on; add more debug keys for different blocks\n",
        "    Debug=0, # 1-debug mode on; add more debug keys for different blocks\n",
        "    #\n",
        "    Debug_DataSet=1,\n",
        "    Debug_Model=1\n",
        "    #\n",
        "    # Debug=0\n",
        ")"
      ],
      "metadata": {
        "id": "-LIQ-g0TG55N"
      },
      "id": "-LIQ-g0TG55N",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if CKeys['Debug']==1:\n",
        "    # add some\n",
        "    CKeys['SlientRun']=0 # 1-save figure into files; 0-show figures\n",
        "    # detailed debug keys\n",
        "    # 1. for model dimension\n",
        "    CKeys['Debug_DataPack']=1\n",
        "    CKeys['Debug_ModelPack']=1\n",
        "    CKeys['Debug_TrainerPack']=1\n",
        "    # 2. for trainer part\n",
        "    CKeys['Debug_DataPack']=1\n",
        "    CKeys['Debug_ModelPack']=0\n",
        "    CKeys['Debug_TrainerPack']=3\n",
        "    # #\n",
        "    # CKeys['testratio']=0.15 # for small ForcPath problem\n",
        "    CKeys['testratio']=0.10 # for large ForcPath problem\n",
        "\n",
        "else:\n",
        "    # for real working run\n",
        "    CKeys['SlientRun']=1\n",
        "    #\n",
        "    CKeys['Debug_DataPack'] = 0\n",
        "    CKeys['Debug_ModelPack'] = 0\n",
        "    CKeys['Debug_TrainerPack'] = 0 # 2 # 1\n",
        "    # add some for training\n",
        "    CKeys['epochs'] = 4000-3250 # 1000 # 200\n",
        "    CKeys['print_loss_every_this_epochs']=50 # 5\n",
        "    CKeys['sample_every_this_epochs']=100 # 50 # 20\n",
        "    CKeys['save_model_every_this_epochs']=50 # 20\n",
        "    # #\n",
        "    # # add some for training\n",
        "    # CKeys['epochs'] = 2000 # 1000 # 200\n",
        "    # CKeys['print_loss_every_this_epochs']=20 # 5\n",
        "    # CKeys['sample_every_this_epochs']=50 # 50 # 20\n",
        "    # CKeys['save_model_every_this_epochs']=50 # 20\n",
        "    #\n",
        "    # CKeys['testratio']=0.15 # for small ForcPath problem\n",
        "    CKeys['testratio']=0.10 # for large ForcPath problem\n",
        "    #\n",
        "    # # add some for training\n",
        "    # CKeys['epochs'] = 4 # 1000 # 200\n",
        "    # CKeys['print_loss_every_this_epochs']=1 # 5\n",
        "    # CKeys['sample_every_this_epochs']=1 # 50 # 20\n",
        "    # CKeys['save_model_every_this_epochs']=2 # 20\n",
        "\n",
        "# for check\n",
        "print(json.dumps(CKeys, indent=4))\n"
      ],
      "metadata": {
        "id": "PKlYp37MG58N",
        "outputId": "610163ee-0f01-4c53-9b4c-b710dae8eb1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PKlYp37MG58N",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Running_Type\": 2,\n",
            "    \"Working_Mode\": 2,\n",
            "    \"IF_FirstRun\": 2,\n",
            "    \"Problem_ID\": 11,\n",
            "    \"Debug\": 0,\n",
            "    \"Debug_DataSet\": 1,\n",
            "    \"Debug_Model\": 1,\n",
            "    \"SlientRun\": 1,\n",
            "    \"Debug_DataPack\": 0,\n",
            "    \"Debug_ModelPack\": 0,\n",
            "    \"Debug_TrainerPack\": 0,\n",
            "    \"epochs\": 750,\n",
            "    \"print_loss_every_this_epochs\": 50,\n",
            "    \"sample_every_this_epochs\": 100,\n",
            "    \"save_model_every_this_epochs\": 50,\n",
            "    \"testratio\": 0.1\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Problem type:\n",
        "print('Problem type: ', CKeys['Problem_ID'])\n",
        "print('Debug mode: ', CKeys['Debug'])\n",
        "print('Working mode: ', CKeys['Working_Mode'])\n"
      ],
      "metadata": {
        "id": "GO-CXtgKG5_E",
        "outputId": "841b8647-794c-41fc-cccb-84b45b1cf67f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GO-CXtgKG5_E",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem type:  11\n",
            "Debug mode:  0\n",
            "Working mode:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===========================================================\n",
        "# Parameter key:\n",
        "# ===========================================================\n",
        "#\n",
        "PKeys = {}\n",
        "# define this one according the running environment\n",
        "# add more if needed\n",
        "# if CKeys['Running_Type']==1 or CKeys['Running_Type']==4:\n",
        "# if CKeys['Running_Type']==1:\n",
        "#\n",
        "#\n",
        "root_path = '/home/gridsan/bni/Test_ground/jupyter/1_git_project/sort_pdb_database_0/Local_Store/'\n",
        "if CKeys['Debug']==1:\n",
        "    # PKeys['prefix']='../Local_Store/For_16_0/'\n",
        "    # use the absolute path for transformability\n",
        "    PKeys['prefix']=root_path+'For_20_0/'\n",
        "\n",
        "if CKeys['Debug']!=1:\n",
        "    # PKeys['prefix']='../Local_Store/For_16_1/'\n",
        "    PKeys['prefix']=root_path+'For_20_1/'\n",
        "    PKeys['prefix']='/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/'\n",
        "    # add one for google colab\n",
        "    PKeys['prefix']=this_working_path # '/content/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/'\n",
        "\n",
        "# store the data pack after processing\n",
        "PKeys['pk_data_pack']=PKeys['prefix']+'data_pack.pickle'\n",
        "PKeys['pk_model_pack']=PKeys['prefix']+'model_pack.pickle'\n",
        "# PKeys[]"
      ],
      "metadata": {
        "id": "6pEl__x-IWsb"
      },
      "id": "6pEl__x-IWsb",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(CKeys['Running_Type'])\n",
        "# print(CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1)\n",
        "# print(PKeys['prefix'])"
      ],
      "metadata": {
        "id": "fDOFHpDTQgSf"
      },
      "id": "fDOFHpDTQgSf",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#\n",
        "# clean EVERYTHING in the dir if 1st\n",
        "#\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "    if os.path.exists(PKeys['prefix']):\n",
        "        cmd_line=f\"rm -r {PKeys['prefix']}\"\n",
        "        print(\"clean the slade...\")\n",
        "        print(f\"excute {cmd_line}\")\n",
        "        os.popen(cmd_line).read()\n",
        "        #\n",
        "    # create dir for working space\n",
        "    UtilityPack.create_path(PKeys['prefix'])"
      ],
      "metadata": {
        "id": "oPNALmfqQgW4"
      },
      "id": "oPNALmfqQgW4",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# prepare the csv files\n",
        "# ========================================================================\n",
        "\n",
        "if CKeys['Problem_ID']==1 or CKeys['Problem_ID']==3 \\\n",
        "or CKeys['Problem_ID']==5 or CKeys['Problem_ID']==7:\n",
        "#     SS_csv_file = PKeys['prefix']+'PROTEIN_Mar18_2022_SECSTR_ALL.csv'\n",
        "\n",
        "#     file_exists = os.path.exists(SS_csv_file)\n",
        "#     if not (file_exists):\n",
        "#         print('Downing the csv file...')\n",
        "#         print(os.popen(f\"wget https://www.dropbox.com/s/7o7s15w9qr6z76y/PROTEIN_Mar18_2022_SECSTR_ALL.csv -P {PKeys['prefix']}\").read())\n",
        "#         print('Done.')\n",
        "#     else:\n",
        "#         print(\"Already there\")\n",
        "    # +\n",
        "    # SS_csv_file = '/home/gridsan/bni/Test_ground/jupyter/1_git_project/sort_pdb_database_0/Local_Store/'+'PROTEIN_Mar18_2022_SECSTR_ALL.csv'\n",
        "    SS_csv_file = root_path+'Local_Store/'+'PROTEIN_Mar18_2022_SECSTR_ALL.csv'\n",
        "\n",
        "if CKeys['Problem_ID']==2 or CKeys['Problem_ID']==4 \\\n",
        "or CKeys['Problem_ID']==6 or CKeys['Problem_ID']==8 \\\n",
        "or CKeys['Problem_ID']==11:\n",
        "    # to be copied locally\n",
        "    # MD_smo_csv_file = '/home/gridsan/bni/Test_ground/jupyter/1_git_project/sort_pdb_database_0/Local_Store/For_1/ForTrain_recon_BSDB_LE_64_smd_disp_forc_df_smo.csv'\n",
        "    # first debug with LE_64, to be update into LE_128\n",
        "    MD_smo_csv_file = root_path+'For_1/ForTrain_recon_BSDB_LE_64_smd_disp_forc_df_smo.csv'\n",
        "    MD_smo_pk_file  = root_path+'For_1/ForTrain_recon_BSDB_LE_64_smd_disp_forc_df_smo_shared.pk'\n",
        "    # into LE_128\n",
        "    MD_smo_pk_file  = root_path+'For_1/ForTrain_recon_BSDB_LE_128_smd_disp_forc_df_smo.pk'"
      ],
      "metadata": {
        "id": "R9mD4zWwQgal"
      },
      "id": "R9mD4zWwQgal",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Handle the dataset"
      ],
      "metadata": {
        "id": "KExbXfH-Q8Qt"
      },
      "id": "KExbXfH-Q8Qt"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rw092iPzQgdS"
      },
      "id": "rw092iPzQgdS",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import PD_pLMProbXDiff.DataSetPack as DataSetPack\n",
        "# importlib.reload(DataSetPack)"
      ],
      "metadata": {
        "id": "PAhbQEOLQgfr"
      },
      "id": "PAhbQEOLQgfr",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('On Problem: ', CKeys['Problem_ID'])"
      ],
      "metadata": {
        "id": "8Akvo5jPQgnJ",
        "outputId": "e4e870f7-1af2-4141-b1ff-ad741114bddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8Akvo5jPQgnJ",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On Problem:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(this_working_path)"
      ],
      "metadata": {
        "id": "OZW2AdRJIWwE",
        "outputId": "fae77dde-90ff-4cb3-c123-7baa2f939cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OZW2AdRJIWwE",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Trained_model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on the sec_str csv file: SecStr\n",
        "# try to convey all para via one key\n",
        "# ====================================================\n",
        "# add some new keys for dataset\n",
        "# ====================================================\n",
        "# for data washing: only for 1st training cycle\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "\n",
        "    if CKeys['Problem_ID']==1:\n",
        "        pass\n",
        "        # print(\"1\")\n",
        "        # # +++++++++++++++++++++++++++++++++++++\n",
        "        # # SecStr as input seq\n",
        "        # DataKeys={}\n",
        "        # DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS/'\n",
        "        # # screening rules\n",
        "        # DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=128\n",
        "        # # X and Y processing\n",
        "        # DataKeys['Xnormfac']=9.\n",
        "        # DataKeys['ynormfac']=21.\n",
        "        # DataKeys['tokenizer_X']=None\n",
        "        # DataKeys['tokenizer_y']=None\n",
        "        # # + for AA embending using ESM\n",
        "        # DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # # deliver\n",
        "        # DataKeys['batch_size']=256\n",
        "        # DataKeys['batch_size']=200\n",
        "        # DataKeys['testset_ratio']=0.1\n",
        "        # DataKeys['maxdata']=99999999999999999\n",
        "        # # add the folder for Data part\n",
        "        # UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "    elif CKeys['Problem_ID']==2:\n",
        "        pass\n",
        "        print(\"2\")\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # MD record as the input seq\n",
        "#         #\n",
        "#         # try to convey all para via one key\n",
        "#         DataKeys={}\n",
        "#         # ======================================\n",
        "#         # keys for \"screen_dataset_MD\"\n",
        "#         DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "#         # add the folder\n",
        "#         UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "#         # screening rules\n",
        "#         DataKeys['min_AA_seq_len']=0\n",
        "#         DataKeys['max_AA_seq_len']=64\n",
        "#         DataKeys['max_Force_cap']=1000\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             file_path=MD_smo_csv_file,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "\n",
        "#         # save the dataframe\n",
        "#         pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "#         pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "#         # ======================================\n",
        "#         # keys for 2nd function\n",
        "#         DataKeys['X_Key']='sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "#         #\n",
        "#         DataKeys['tokenizer_X']=None # will not be used\n",
        "#         DataKeys['tokenizer_y']=None # to be created\n",
        "#         DataKeys['Xnormfac'] = np.max(protein_df['Max_Smo_Force'])\n",
        "#         print('Normalization factor for force: ', DataKeys['Xnormfac'])\n",
        "#         DataKeys['ynormfac']=21. # old force diffusion model 22.\n",
        "#         #\n",
        "#         DataKeys['batch_size']=256\n",
        "#         DataKeys['testset_ratio']=0.15\n",
        "#         DataKeys['maxdata']=99999999991000\n",
        "\n",
        "    elif CKeys['Problem_ID']==3:\n",
        "        pass\n",
        "        # print(\"3\")\n",
        "        # # +++++++++++++++++++++++++++++++++++++\n",
        "        # # SecStr as input seq\n",
        "        # DataKeys={}\n",
        "        # DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS_ModelA/'\n",
        "        # # screening rules\n",
        "        # DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64 # 128\n",
        "        # DataKeys['max_text_len']=8\n",
        "        # # X and Y processing\n",
        "        # DataKeys['Xnormfac']=1.\n",
        "        # DataKeys['ynormfac']=22. # 21.\n",
        "        # DataKeys['tokenizer_X']=None\n",
        "        # DataKeys['tokenizer_y']=None\n",
        "        # # deliver\n",
        "        # DataKeys['batch_size']=512\n",
        "        # # for debug purpose\n",
        "        # # DataKeys['batch_size']=1\n",
        "        # DataKeys['testset_ratio']= 0.1\n",
        "        # DataKeys['maxdata']=99999999999999999\n",
        "        # # add the folder for Data part\n",
        "        # UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "    elif CKeys['Problem_ID']==4:\n",
        "        pass\n",
        "\n",
        "#         print(\"4: input text condition, output sequence...\")\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # MD record as the input seq\n",
        "#         #\n",
        "#         # try to convey all para via one key\n",
        "#         DataKeys={}\n",
        "#         # ======================================\n",
        "#         # keys for \"screen_dataset_MD\"\n",
        "#         DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "#         # add the folder\n",
        "#         UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "#         # screening rules\n",
        "#         DataKeys['min_AA_seq_len']=0\n",
        "#         DataKeys['max_AA_seq_len']=64\n",
        "#         DataKeys['max_text_len']=2\n",
        "#         DataKeys['max_Force_cap']=1000\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             file_path=MD_smo_csv_file,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "\n",
        "#         # save the dataframe\n",
        "#         pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "#         pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "#         # ======================================\n",
        "#         # keys for 2nd function\n",
        "#         DataKeys['X_Key']=['Max_Smo_Force','Int_Smo_ForcPull'] # 'sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "#         #\n",
        "#         DataKeys['tokenizer_X']=None # will not be used\n",
        "#         DataKeys['tokenizer_y']=None # to be created\n",
        "#         #\n",
        "#         print('Normalization factor for force: ',\n",
        "#               np.max(protein_df['Max_Smo_Force']))\n",
        "#         print('Normalization factor for toughness: ',\n",
        "#               np.max(protein_df['Int_Smo_ForcPull']))\n",
        "#         #\n",
        "#         DataKeys['Xnormfac'] = np.array([\n",
        "#             np.max(protein_df['Max_Smo_Force']),\n",
        "#             np.max(protein_df['Int_Smo_ForcPull'])\n",
        "#         ])\n",
        "#         #\n",
        "#         DataKeys['ynormfac']=21. # old force diffusion model 22.\n",
        "#         #\n",
        "#         DataKeys['batch_size']=256\n",
        "#         DataKeys['testset_ratio']=0.15\n",
        "#         DataKeys['maxdata']=99999999991000\n",
        "\n",
        "    # /////////////////////////////////////////////////////////////\n",
        "    # try embedding\n",
        "    # \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "    elif CKeys['Problem_ID']==5:\n",
        "        pass\n",
        "\n",
        "#         print(\"5\")\n",
        "#         # +++++++++++++++++++++++++++++++++++++\n",
        "#         # SecStr as input seq\n",
        "#         DataKeys={}\n",
        "#         DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS/'\n",
        "#         # screening rules\n",
        "#         DataKeys['min_AA_seq_len']=0\n",
        "#         DataKeys['max_AA_seq_len']=128\n",
        "#         # X and Y processing\n",
        "#         DataKeys['Xnormfac']=9.\n",
        "#         DataKeys['ynormfac']=1 # for ESM # 21.\n",
        "#         DataKeys['tokenizer_X']=None\n",
        "#         DataKeys['tokenizer_y']=None\n",
        "#         # + for AA embending using ESM\n",
        "#         DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "#         # add for embedding space\n",
        "#         DataKeys['image_channels']=1280\n",
        "#         # deliver\n",
        "#         DataKeys['batch_size']=256\n",
        "#         DataKeys['batch_size']=256 # 0 # 200\n",
        "#         DataKeys['testset_ratio']=CKeys['testratio'] # 0.1\n",
        "#         DataKeys['maxdata']=99999999999999999\n",
        "\n",
        "#         # add the folder for Data part\n",
        "#         UtilityPack.create_path(DataKeys['data_dir'])\n",
        "    #\n",
        "    elif CKeys['Problem_ID']==6:\n",
        "        print(\"6, input MD ForcePath, output AA sequence\")\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # MD record as the tokenized input seq\n",
        "        #\n",
        "        # try to convey all para via one key\n",
        "        DataKeys={}\n",
        "        # ======================================\n",
        "        # keys for \"screen_dataset_MD\"\n",
        "        DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "        # add the folder\n",
        "        UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "        # screening rules\n",
        "        DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64\n",
        "        DataKeys['max_AA_seq_len']=128\n",
        "        DataKeys['max_Force_cap']=1000\n",
        "#         # ---------------------------------------\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             csv_file=MD_smo_csv_file,\n",
        "#             pk_file=None,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "            csv_file=None,\n",
        "            pk_file=MD_smo_pk_file,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "        # save the dataframe\n",
        "        pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "        pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "        # ======================================\n",
        "        # keys for 2nd function\n",
        "        DataKeys['X_Key']='sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "        #\n",
        "        DataKeys['tokenizer_X']=None # will not be used\n",
        "        DataKeys['tokenizer_y']=None # to be created\n",
        "        # think about this: update this one if necessary\n",
        "        # DataKeys['Xnormfac'] = np.max(protein_df['Max_Smo_Force'])\n",
        "        DataKeys['Xnormfac'] = 750.\n",
        "\n",
        "        print('Normalization factor for force: ', DataKeys['Xnormfac'])\n",
        "        DataKeys['ynormfac']=1. # not used as esm is used # 21. # old force diffusion model 22.\n",
        "        #\n",
        "        DataKeys['batch_size']=256\n",
        "        DataKeys['testset_ratio']=0.15\n",
        "        DataKeys['maxdata']=99999999991000\n",
        "        # ++ for pLM\n",
        "        # for AA embending using ESM\n",
        "        DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # add for embedding space\n",
        "        DataKeys['image_channels']=1280\n",
        "        #\n",
        "        DataKeys['ESM-2_Model']='esm2_t12_35M_UR50D'\n",
        "        DataKeys['image_channels']=480\n",
        "\n",
        "\n",
        "    elif CKeys['Problem_ID']==7:\n",
        "        pass\n",
        "        # print(\"7\")\n",
        "        # # +++++++++++++++++++++++++++++++++++++\n",
        "        # # SecStr text as input seq\n",
        "        # DataKeys={}\n",
        "        # DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS_ModelA/'\n",
        "        # # screening rules\n",
        "        # DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64 # 128\n",
        "        # DataKeys['max_text_len']=8\n",
        "        # # X and Y processing\n",
        "        # DataKeys['Xnormfac']=1.\n",
        "        # DataKeys['ynormfac']=1. # for ESM # 21. 22. # 21.\n",
        "        # DataKeys['tokenizer_X']=None\n",
        "        # DataKeys['tokenizer_y']=None\n",
        "        # # deliver\n",
        "        # DataKeys['batch_size']=512\n",
        "        # # for debug purpose\n",
        "        # # DataKeys['batch_size']=1\n",
        "        # DataKeys['testset_ratio']= CKeys['testratio'] # 0.1\n",
        "        # DataKeys['maxdata']=99999999999999999\n",
        "        # # + for AA embending using ESM\n",
        "        # DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # # add for embedding space\n",
        "        # DataKeys['image_channels']=1280\n",
        "        # #\n",
        "        # # add the folder for Data part\n",
        "        # UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "    elif CKeys['Problem_ID']==8:\n",
        "        #\n",
        "        print(\"8: input text condition, output sequence...\")\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # MD record as the input seq\n",
        "        #\n",
        "        # try to convey all para via one key\n",
        "        DataKeys={}\n",
        "        # ======================================\n",
        "        # keys for \"screen_dataset_MD\"\n",
        "        DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "        # add the folder\n",
        "        UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "        # screening rules\n",
        "        DataKeys['min_AA_seq_len']=0\n",
        "        DataKeys['max_AA_seq_len']=64\n",
        "        DataKeys['max_AA_seq_len']=128\n",
        "        DataKeys['max_text_len']=2\n",
        "        DataKeys['max_Force_cap']=1000\n",
        "#         # ---------------------------------------------------------\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             # # --\n",
        "#             # file_path=MD_smo_csv_file,\n",
        "#             # ++\n",
        "#             csv_file=MD_smo_csv_file,\n",
        "#             pk_file=None,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "            csv_file=None,\n",
        "            pk_file=MD_smo_pk_file,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "        # save the dataframe\n",
        "        pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "        pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "        # ======================================\n",
        "        # keys for 2nd function\n",
        "        DataKeys['X_Key']=['Max_Smo_Force','Int_Smo_ForcPull'] # 'sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "        #\n",
        "        DataKeys['tokenizer_X']=None # will not be used\n",
        "        DataKeys['tokenizer_y']=None # will not be used # to be created\n",
        "        #\n",
        "        print('Normalization factor for force: ',\n",
        "              np.max(protein_df['Max_Smo_Force']))\n",
        "        print('Normalization factor for toughness: ',\n",
        "              np.max(protein_df['Int_Smo_ForcPull']))\n",
        "        #\n",
        "        DataKeys['Xnormfac'] = np.array([\n",
        "            np.max(protein_df['Max_Smo_Force']),\n",
        "            np.max(protein_df['Int_Smo_ForcPull'])\n",
        "        ])\n",
        "        #\n",
        "        DataKeys['ynormfac']=1.0 # not used in esm # 21. # old force diffusion model 22.\n",
        "        #\n",
        "        DataKeys['batch_size']=256\n",
        "        DataKeys['testset_ratio']=CKeys['testratio'] # 0.15\n",
        "        DataKeys['maxdata']=99999999991000\n",
        "        # + for AA embending using ESM\n",
        "        DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # add for embedding space\n",
        "        DataKeys['image_channels']=1280\n",
        "\n",
        "    elif CKeys['Problem_ID']==11:\n",
        "        # copied from Problem_ID=6\n",
        "        #\n",
        "        print(\"11, input MD ForcePath, output AA sequence\")\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # MD record as the tokenized input seq\n",
        "        #\n",
        "        # try to convey all para via one key\n",
        "        DataKeys={}\n",
        "        # ======================================\n",
        "        # keys for \"screen_dataset_MD\"\n",
        "        DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "        # add the folder\n",
        "        UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "        # screening rules\n",
        "        DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64\n",
        "        DataKeys['max_AA_seq_len']=128\n",
        "        DataKeys['max_Force_cap']=1000\n",
        "#         # ---------------------------------------\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             csv_file=MD_smo_csv_file,\n",
        "#             pk_file=None,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "            csv_file=None,\n",
        "            pk_file=MD_smo_pk_file,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "        # save the dataframe\n",
        "        pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "        pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "        # ======================================\n",
        "        # keys for 2nd function\n",
        "        DataKeys['X_Key']='sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "        #\n",
        "        DataKeys['tokenizer_X']=None # will not be used\n",
        "        DataKeys['tokenizer_y']=None # to be created\n",
        "        # think about this: update this one if necessary\n",
        "        # DataKeys['Xnormfac'] = np.max(protein_df['Max_Smo_Force'])\n",
        "        DataKeys['Xnormfac'] = 750.\n",
        "\n",
        "        print('Normalization factor for force: ', DataKeys['Xnormfac'])\n",
        "        DataKeys['ynormfac']=1. # not used as esm is used # 21. # old force diffusion model 22.\n",
        "        #\n",
        "        DataKeys['batch_size']=256\n",
        "        DataKeys['testset_ratio']=0.15\n",
        "        DataKeys['maxdata']=99999999991000\n",
        "        # ++ for pLM\n",
        "        # for AA embending using ESM\n",
        "        DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # add for embedding space\n",
        "        # DataKeys['image_channels']=1280\n",
        "        #\n",
        "        # DataKeys['ESM-2_Model']='esm2_t12_35M_UR50D'\n",
        "        # # DataKeys['image_channels']=480\n",
        "        #\n",
        "        # DataKeys['ESM-2_Model']='esm2_t36_3B_UR50D'\n",
        "        # DataKeys['image_channels']=2560\n",
        "        #\n",
        "        DataKeys['ESM-2_Model']='esm2_t30_150M_UR50D'\n",
        "        # DataKeys['image_channels']=640\n",
        "\n",
        "        # only use the probability part\n",
        "        DataKeys['image_channels']=33\n",
        "\n",
        "    else:\n",
        "        print('No Problem Type found...')\n",
        "# else:\n",
        "#     # load back if there is anything generated in the 1st run\n",
        "#     if CKeys['Problem_ID']==2 or CKeys['Problem_ID']==6:\n",
        "#         protein_df = pd.read_pickle(DataKeys['data_dir']+'protein_df.pk')\n",
        "#         df_raw = pd.read_pickle(DataKeys['data_dir']+'df_raw.pk')"
      ],
      "metadata": {
        "id": "msXLgSXqIWzi"
      },
      "id": "msXLgSXqIWzi",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(CKeys)"
      ],
      "metadata": {
        "id": "2GhYVnk2IW3l",
        "outputId": "eeec5821-d767-4a26-e90d-4c46ba1b7476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2GhYVnk2IW3l",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Running_Type': 2, 'Working_Mode': 2, 'IF_FirstRun': 2, 'Problem_ID': 11, 'Debug': 0, 'Debug_DataSet': 1, 'Debug_Model': 1, 'SlientRun': 1, 'Debug_DataPack': 0, 'Debug_ModelPack': 0, 'Debug_TrainerPack': 0, 'epochs': 750, 'print_loss_every_this_epochs': 50, 'sample_every_this_epochs': 100, 'save_model_every_this_epochs': 50, 'testratio': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ====================================================\n",
        "# convert into datasets\n",
        "# ====================================================\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "    if CKeys['Problem_ID']==1:\n",
        "        pass\n",
        "        # train_loader, \\\n",
        "        # train_loader_noshuffle, \\\n",
        "        # test_loader, \\\n",
        "        # tokenizer_y, tokenizer_X = DataSetPack.load_data_set_SS_InSeqToOuSeq(\n",
        "        #     file_path=SS_csv_file,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==2:\n",
        "        pass\n",
        "        # train_loader, train_loader_noshuffle, \\\n",
        "        # test_loader, tokenizer_y, tokenizer_X = DataSetPack.load_data_set_from_df_SMD(\n",
        "        #     protein_df,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==3:\n",
        "        pass\n",
        "        # train_loader, train_loader_noshuffle, \\\n",
        "        # test_loader,tokenizer_y, tokenizer_X = DataSetPack.load_data_set_seq2seq_SecStr_ModelA (\n",
        "        #     file_path=SS_csv_file, # 'PROTEIN_Mar18_2022_SECSTR_ALL.csv',\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==4:\n",
        "        pass\n",
        "        # train_loader, train_loader_noshuffle, \\\n",
        "        # test_loader,tokenizer_y, tokenizer_X = DataSetPack.load_data_set_text2seq_MD_ModelA (\n",
        "        #     protein_df,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    # ///////////////////////////////////////////////////////////////\n",
        "    #  add embedding cases\n",
        "    # \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "    elif CKeys['Problem_ID']==5:\n",
        "        pass\n",
        "        # train_loader, \\\n",
        "        # train_loader_noshuffle, \\\n",
        "        # test_loader, \\\n",
        "        # tokenizer_y, \\\n",
        "        # tokenizer_X = DataSetPack.load_data_set_SS_InSeqToOuSeq_pLM(\n",
        "        #     file_path=SS_csv_file,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "        # # this will triger the following downloading\n",
        "        # # Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /home/gridsan/bni/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
        "        # # excute the following if without internet on the node\n",
        "        # # 1 $ wget https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt -O  /home/gridsan/bni/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
        "        # # 2 $ wget https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt -O /home/gridsan/bni/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n",
        "\n",
        "    # add if needed\n",
        "    elif CKeys['Problem_ID']==6:\n",
        "\n",
        "        train_loader, \\\n",
        "        train_loader_noshuffle, \\\n",
        "        test_loader, \\\n",
        "        tokenizer_y, \\\n",
        "        tokenizer_X = DataSetPack.load_data_set_from_df_SMD_pLM(\n",
        "                protein_df,\n",
        "                PKeys=DataKeys, # to be updated\n",
        "                CKeys=CKeys,\n",
        "            )\n",
        "\n",
        "    elif CKeys['Problem_ID']==7:\n",
        "        pass\n",
        "        # train_loader, \\\n",
        "        # train_loader_noshuffle, \\\n",
        "        # test_loader, \\\n",
        "        # tokenizer_y, \\\n",
        "        # tokenizer_X = DataSetPack.load_data_set_seq2seq_SecStr_ModelA_pLM (\n",
        "        #     file_path=SS_csv_file, # 'PROTEIN_Mar18_2022_SECSTR_ALL.csv',\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==8:\n",
        "\n",
        "        train_loader, \\\n",
        "        train_loader_noshuffle, \\\n",
        "        test_loader,\\\n",
        "        tokenizer_y, \\\n",
        "        tokenizer_X = DataSetPack.load_data_set_text2seq_MD_ModelA_pLM (\n",
        "            protein_df,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "    elif CKeys['Problem_ID']==11:\n",
        "\n",
        "        train_loader, \\\n",
        "        train_loader_noshuffle, \\\n",
        "        test_loader, \\\n",
        "        tokenizer_y, \\\n",
        "        tokenizer_X = DataSetPack.load_data_set_from_df_SMD_pLM(\n",
        "                protein_df,\n",
        "                PKeys=DataKeys, # to be updated\n",
        "                CKeys=CKeys,\n",
        "            )\n",
        "\n",
        "    elif CKeys['Problem_ID']==12:\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        print('No Problem Type found...')\n",
        "\n",
        "    print(\"==========================================\")\n",
        "    print(\"Save the datasets ...\")\n",
        "    print(\"==========================================\")\n",
        "    # save the dataset for for the 1st run\n",
        "    data_pack = {}\n",
        "    data_pack['train_loader']=train_loader\n",
        "    data_pack['train_loader_noshuffle']=train_loader_noshuffle\n",
        "    data_pack['test_loader']=test_loader\n",
        "    data_pack['tokenizer_X']=tokenizer_X\n",
        "    data_pack['tokenizer_y']=tokenizer_y\n",
        "    # keys\n",
        "    data_pack['DataKeys']=DataKeys\n",
        "    # data_pack['CKeys']=CKeys\n",
        "    data_pack['PKeys']=PKeys\n",
        "    with open(PKeys['pk_data_pack'], 'wb') as handle:\n",
        "        pickle.dump(data_pack, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "else: # work both for training and testing\n",
        "\n",
        "    print('This is not the first run')\n",
        "    print('Load back in the data packages...')\n",
        "    with open(PKeys['pk_data_pack'], 'rb') as handle:\n",
        "        data_pack = pickle.load(handle)\n",
        "    # deliver the results\n",
        "    train_loader=data_pack['train_loader']\n",
        "    train_loader_noshuffle=data_pack['train_loader_noshuffle']\n",
        "    test_loader=data_pack['test_loader']\n",
        "    tokenizer_X=data_pack['tokenizer_X']\n",
        "    tokenizer_y=data_pack['tokenizer_y']\n",
        "    # keys (create or update)\n",
        "    DataKeys=data_pack['DataKeys']\n",
        "    # CKeys=data_pack['CKeys']\n",
        "    PKeys=data_pack['PKeys']\n",
        "    # add some for specific problem\n",
        "    if CKeys['Problem_ID']==2 or CKeys['Problem_ID']==6 \\\n",
        "    or CKeys['Problem_ID']==11:\n",
        "        protein_df = pd.read_pickle(DataKeys['data_dir']+'protein_df.pk')\n",
        "        df_raw = pd.read_pickle(DataKeys['data_dir']+'df_raw.pk')\n",
        "    print('Done.')\n",
        "\n"
      ],
      "metadata": {
        "id": "ExgiEEdLRZCP",
        "outputId": "42cf2d66-0824-4c06-db98-717ee5ab18ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "id": "ExgiEEdLRZCP",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is not the first run\n",
            "Load back in the data packages...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-c67deef4993e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load back in the data packages...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPKeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pk_data_pack'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;31m# deliver the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_pack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PD_pLMProbXDiff.DataSetPack'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL7flQUoRZFe"
      },
      "id": "VL7flQUoRZFe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mo8scfhnRZLe"
      },
      "id": "Mo8scfhnRZLe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxFqwGM5RZOS"
      },
      "id": "bxFqwGM5RZOS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PKeys['pk_data_pack'])"
      ],
      "metadata": {
        "id": "MPk7OefFRZRA",
        "outputId": "f2d3c2d9-d0a4-40d8-d5d1-4e0d0fab5cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MPk7OefFRZRA",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Trained_model/data_pack.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(PKeys['pk_data_pack'])\n"
      ],
      "metadata": {
        "id": "AQ2dE_8vARaK",
        "outputId": "e236055d-c8f7-4371-f92b-61220ccc3bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AQ2dE_8vARaK",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Trained_model/data_pack.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fk5K2mKt_d7_"
      },
      "id": "fk5K2mKt_d7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ModelPack"
      ],
      "metadata": {
        "id": "YX9tFYEZ8aMb"
      },
      "id": "YX9tFYEZ8aMb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78f755b5-7c61-48a9-878d-ab01cab4baf6",
      "metadata": {
        "id": "78f755b5-7c61-48a9-878d-ab01cab4baf6",
        "outputId": "2950dd61-fa2e-4079-ed0e-0ccd4c605c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ],
      "source": [
        "print('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a1e936-2879-4ac6-add6-718482b9ca38",
      "metadata": {
        "id": "76a1e936-2879-4ac6-add6-718482b9ca38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}