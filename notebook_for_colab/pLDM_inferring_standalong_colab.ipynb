{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3faa7771-bb3a-4024-9c4b-9067bb2e0f17",
      "metadata": {
        "id": "3faa7771-bb3a-4024-9c4b-9067bb2e0f17"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Bo-Ni/ProteinMechanicsDiffusionDesign_pLDM/blob/main/notebook_for_colab/pLDM_inferring_standalong_colab.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Preparation:\n",
        "### 0-1. Add packages"
      ],
      "metadata": {
        "id": "xQvn0r7NYIgW"
      },
      "id": "xQvn0r7NYIgW"
    },
    {
      "cell_type": "code",
      "source": [
        "# !python --version"
      ],
      "metadata": {
        "id": "YvjMugf4BE0U"
      },
      "id": "YvjMugf4BE0U",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install python3.9"
      ],
      "metadata": {
        "id": "wwtMjKUBAkGO"
      },
      "id": "wwtMjKUBAkGO",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.pop(0)\n",
        "# sys.path.insert(0, '/usr/bin/python3.9')"
      ],
      "metadata": {
        "id": "0Og83AHiAz8h"
      },
      "id": "0Og83AHiAz8h",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys\n",
        "import math\n",
        "\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #turn off CUDA if needed\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "path_1 = '/opt/bin/'\n",
        "dssp_file = path_1+'mkdssp'\n",
        "\n",
        "file_exists = os.path.exists(dssp_file)\n",
        "if not (file_exists):\n",
        "  print('\\033[1;32m For the 1st run, ')\n",
        "  # ==============================================\n",
        "  print('a. Install omegafold...')\n",
        "  # install omegafold\n",
        "  # ref: https://github.com/HeliXonProtein/OmegaFold\n",
        "  !pip install git+https://github.com/HeliXonProtein/OmegaFold.git\n",
        "\n",
        "  # time-consuming step:\n",
        "  # Downloading weights from https://helixon.s3.amazonaws.com/release1.pt to /root/.cache/omegafold_ckpt/model.pt\n",
        "  !mkdir /root/.cache/omegafold_ckpt\n",
        "  !wget https://helixon.s3.amazonaws.com/release1.pt -O /root/.cache/omegafold_ckpt/model.pt\n",
        "\n",
        "  print('b. Install DSSP...')\n",
        "  # download an mkdssp\n",
        "  # ==============================================\n",
        "  # download things\n",
        "  print(os.popen(f\"wget https://www.dropbox.com/s/v4azy9z9yojg1c6/mkdssp -P {path_1}\").read())\n",
        "  #\n",
        "  !chmod u+x /opt/bin/mkdssp\n",
        "\n",
        "else:\n",
        "  print('This is not the first run... ')"
      ],
      "metadata": {
        "id": "cLDyOsEcYYsy",
        "outputId": "cf8573f1-1daf-47f0-d9b9-873d6af49b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cLDyOsEcYYsy",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32m For the 1st run, \n",
            "a. Install omegafold...\n",
            "Collecting git+https://github.com/HeliXonProtein/OmegaFold.git\n",
            "  Cloning https://github.com/HeliXonProtein/OmegaFold.git to /tmp/pip-req-build-rtn3m6vg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/HeliXonProtein/OmegaFold.git /tmp/pip-req-build-rtn3m6vg\n",
            "  Resolved https://github.com/HeliXonProtein/OmegaFold.git to commit 313c873ad190b64506a497c926649e15fcd88fcd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch@ https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (from OmegaFold==0.0.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython (from OmegaFold==0.0.0)\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython->OmegaFold==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch@ https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl->OmegaFold==0.0.0) (4.5.0)\n",
            "Building wheels for collected packages: OmegaFold\n",
            "  Building wheel for OmegaFold (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for OmegaFold: filename=OmegaFold-0.0.0-py3-none-any.whl size=55664 sha256=327e601ea86253a594846ea033a4e4c3acfe3208535b460f4e4a09f253c1398d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n70_867g/wheels/fe/57/01/3ce12996dd37debe3ee6a02e8748fffdd4df7b885c5bb8071d\n",
            "Successfully built OmegaFold\n",
            "Installing collected packages: torch, biopython, OmegaFold\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OmegaFold-0.0.0 biopython-1.81 torch-1.12.0+cu113\n",
            "--2023-10-01 04:17:10--  https://helixon.s3.amazonaws.com/release1.pt\n",
            "Resolving helixon.s3.amazonaws.com (helixon.s3.amazonaws.com)... 52.216.206.83, 52.216.218.97, 52.216.36.217, ...\n",
            "Connecting to helixon.s3.amazonaws.com (helixon.s3.amazonaws.com)|52.216.206.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3181611124 (3.0G) [binary/octet-stream]\n",
            "Saving to: ‘/root/.cache/omegafold_ckpt/model.pt’\n",
            "\n",
            "/root/.cache/omegaf 100%[===================>]   2.96G  15.2MB/s    in 1m 52s  \n",
            "\n",
            "2023-10-01 04:19:03 (27.2 MB/s) - ‘/root/.cache/omegafold_ckpt/model.pt’ saved [3181611124/3181611124]\n",
            "\n",
            "b. Install DSSP...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add some packages\n",
        "try:\n",
        "  print('\\033[1;32m a. on Biopython...')\n",
        "  from Bio.PDB import PDBParser\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install biopython').read())\n",
        "\n",
        "try:\n",
        "  print('\\033[1;32m b. on kornia...')\n",
        "  import kornia.augmentation\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install kornia').read())\n",
        "\n",
        "try:\n",
        "  print('\\033[1;32m c. on einops...')\n",
        "  from einops import rearrange, repeat, reduce\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install einops').read())\n",
        "\n",
        "try:\n",
        "  from einops_exts import rearrange_many, repeat_many, check_shape\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install einops-exts').read())\n",
        "\n",
        "try:\n",
        "  import pytorch_warmup as warmup\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install pytorch-warmup').read())\n",
        "\n",
        "try:\n",
        "  from ema_pytorch import EMA\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install ema-pytorch').read())\n",
        "\n",
        "try:\n",
        "  from accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install accelerate').read())\n",
        "\n",
        "try:\n",
        "  import py3Dmol\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install py3Dmol').read())\n",
        "\n",
        "# added\n",
        "try:\n",
        "  import esm\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install fair-esm').read())\n",
        "\n",
        "try:\n",
        "  import torchinfo\n",
        "except ImportError as e:\n",
        "  print(os.popen('pip install torchinfo').read())"
      ],
      "metadata": {
        "id": "pUhFAfNfZFsW",
        "outputId": "58180699-117a-432b-da98-1d40706ac1c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pUhFAfNfZFsW",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32m a. on Biopython...\n",
            "\u001b[1;32m b. on kornia...\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 705.7/705.7 kB 9.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.7.0\n",
            "\n",
            "\u001b[1;32m c. on einops...\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 1.9 MB/s eta 0:00:00\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.7.0\n",
            "\n",
            "Collecting einops-exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.10/dist-packages (from einops-exts) (0.7.0)\n",
            "Installing collected packages: einops-exts\n",
            "Successfully installed einops-exts-0.0.4\n",
            "\n",
            "Collecting pytorch-warmup\n",
            "  Downloading pytorch_warmup-0.1.1-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-warmup) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->pytorch-warmup) (4.5.0)\n",
            "Installing collected packages: pytorch-warmup\n",
            "Successfully installed pytorch-warmup-0.1.1\n",
            "\n",
            "Collecting ema-pytorch\n",
            "  Downloading ema_pytorch-0.2.3-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from ema-pytorch) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->ema-pytorch) (4.5.0)\n",
            "Installing collected packages: ema-pytorch\n",
            "Successfully installed ema-pytorch-0.2.3\n",
            "\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 6.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.12.0+cu113)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 12.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Installing collected packages: huggingface-hub, accelerate\n",
            "Successfully installed accelerate-0.23.0 huggingface-hub-0.17.3\n",
            "\n",
            "Collecting py3Dmol\n",
            "  Downloading py3Dmol-2.0.4-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: py3Dmol\n",
            "Successfully installed py3Dmol-2.0.4\n",
            "\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.1/93.1 kB 3.0 MB/s eta 0:00:00\n",
            "Installing collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2. copy src from github"
      ],
      "metadata": {
        "id": "R3JeHPw9bOMQ"
      },
      "id": "R3JeHPw9bOMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, os, sys, glob\n",
        "\n",
        "# # just script, no install is used\n",
        "# if not os.path.isdir(\"ProteinMPNN\"):\n",
        "#   os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "# sys.path.append('/content/ProteinMPNN/')\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "if not os.path.isdir(\"ProteinMechanicsDiffusionDesign_pLDM\"):\n",
        "  os.system(\"git clone -q https://github.com/Bo-Ni/ProteinMechanicsDiffusionDesign_pLDM.git\")\n",
        "# sys.path.append('/content/ProteinMechanicsDiffusionDesign_pLDM/ProteinMechanicsDiffusionDesign/')\n",
        "sys.path.append('/content/ProteinMechanicsDiffusionDesign_pLDM/')"
      ],
      "metadata": {
        "id": "wV1H9Bf0bYat"
      },
      "id": "wV1H9Bf0bYat",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a slient test\n",
        "# import ProteinMechanicsDiffusionDesign.UtilityPack as UtilityPack\n",
        "# import ProteinMechanicsDiffusionDesign.DataSetPack as DataSetPack\n",
        "# import ProteinMechanicsDiffusionDesign.ModelPack as ModelPack\n",
        "# import ProteinMechanicsDiffusionDesign.TrainerPack as TrainerPack\n",
        "# import ProteinMechanicsDiffusionDesign.PostMDPack as PostMDPack\n",
        "#\n",
        "import PD_pLMProbXDiff.UtilityPack as UtilityPack\n",
        "import PD_pLMProbXDiff.DataSetPack as DataSetPack\n",
        "import PD_pLMProbXDiff.ModelPack as ModelPack\n",
        "import PD_pLMProbXDiff.TrainerPack as TrainerPack\n",
        "import PD_pLMProbXDiff.PostMDPack as PostMDPack"
      ],
      "metadata": {
        "id": "TBNNv-Oo7Ftw",
        "outputId": "538b4490-af3a-4764-b63f-e6fbeae5101a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TBNNv-Oo7Ftw",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104impl8GPUTrace13gpuTraceStateE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "identify the device independently cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3. Download the model files"
      ],
      "metadata": {
        "id": "BRkJe9AUNRfy"
      },
      "id": "BRkJe9AUNRfy"
    },
    {
      "cell_type": "code",
      "source": [
        "# just script, no install is used\n",
        "this_working_path = '/content/working_results/'\n",
        "\n",
        "if not os.path.isdir(this_working_path):\n",
        "  print('Creating working path...')\n",
        "  print(os.popen('mkdir '+this_working_path).read())\n",
        "  print('Done.')\n",
        "  print('Downing files...')\n",
        "\n",
        "this_file = this_working_path+'model_pack.pickle'\n",
        "file_exists = os.path.exists(this_file)\n",
        "if not (file_exists):\n",
        "  # download things\n",
        "  this_link='https://www.dropbox.com/scl/fi/i2sull7ftjwrrzeaxo8v1/model_pack.pickle?rlkey=7wy5zynrl6m8azufklq3fy8ql&dl=0'\n",
        "  cmd_line = f\"wget -O {this_file} {this_link}\"\n",
        "  print(os.popen(cmd_line).read())\n",
        "\n",
        "#\n",
        "this_file = this_working_path+'data_pack.pickle'\n",
        "file_exists = os.path.exists(this_file)\n",
        "if not (file_exists):\n",
        "  # download things\n",
        "  this_link='https://www.dropbox.com/scl/fi/z7sz0q2nsjn85kyh68p86/data_pack.pickle?rlkey=bwm9fgf29ze8o516r155zg4gl&dl=0'\n",
        "  cmd_line = f\"wget -O {this_file} {this_link}\"\n",
        "  print(os.popen(cmd_line).read())\n",
        "\n",
        "# add for dataset part\n",
        "this_temp_path = this_working_path+'0_dataprocess_MD/'\n",
        "if not os.path.isdir(this_temp_path):\n",
        "  print('Creating data path...')\n",
        "  print(os.popen('mkdir '+this_temp_path).read())\n",
        "  print('Done.')\n",
        "  print('Downing files...')\n",
        "# add file if needed\n",
        "\n",
        "\n",
        "# add for model part\n",
        "this_temp_path = this_working_path+'1_model_SS/'\n",
        "if not os.path.isdir(this_temp_path):\n",
        "  print('Creating model path...')\n",
        "  print(os.popen('mkdir '+this_temp_path).read())\n",
        "  print('Done.')\n",
        "  print('Downing files...')\n",
        "# add file if needed\n",
        "#\n",
        "this_file = this_working_path+'1_model_SS/'+'trainer_save-model_pLDM.pt'\n",
        "file_exists = os.path.exists(this_file)\n",
        "if not (file_exists):\n",
        "  # download things\n",
        "  this_link='https://www.dropbox.com/s/09wyap14yrnoeom/trainer_save-model_pLDM.pt?dl=0'\n",
        "  cmd_line = f\"wget -O {this_file} {this_link}\"\n",
        "  print(os.popen(cmd_line).read())"
      ],
      "metadata": {
        "id": "h5l3MrGnNQuJ",
        "outputId": "ab9f24cf-e804-4dd4-eba3-fd1adfddb7ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h5l3MrGnNQuJ",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model path...\n",
            "\n",
            "Done.\n",
            "Downing files...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cmd_line)"
      ],
      "metadata": {
        "id": "V0sTt-9-sBuD",
        "outputId": "f5a52a9e-07b3-4e1d-8c9d-ed22c692e069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V0sTt-9-sBuD",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget -O /content/working_results/1_model_SS/trainer_save-model_pLDM.pt https://www.dropbox.com/s/09wyap14yrnoeom/trainer_save-model_pLDM.pt?dl=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Working part"
      ],
      "metadata": {
        "id": "jc-QOhsnGAm9"
      },
      "id": "jc-QOhsnGAm9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0. Check the floor"
      ],
      "metadata": {
        "id": "frn-YZYnGN51"
      },
      "id": "frn-YZYnGN51"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys"
      ],
      "metadata": {
        "id": "AqtZKiwq_CaO"
      },
      "id": "AqtZKiwq_CaO",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Here is : \\n', os.popen('pwd').read())\n",
        "print('What we get in hardware: \\n', os.popen('nvidia-smi').read())"
      ],
      "metadata": {
        "id": "o6kXrcuX_LtK",
        "outputId": "baf74a5a-5386-440d-b97a-ef4774b7a6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o6kXrcuX_LtK",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is : \n",
            " /content\n",
            "\n",
            "What we get in hardware: \n",
            " Sun Oct  1 04:21:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    43W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"What we have in software: \\n Torch version:\", torch.__version__)\n",
        "print('Python: ', sys.version) # no switch case code"
      ],
      "metadata": {
        "id": "AXDZZWJk_ZUY",
        "outputId": "d73d4574-5ddc-429e-8d51-29fc198003e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AXDZZWJk_ZUY",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What we have in software: \n",
            " Torch version: 1.12.0+cu113\n",
            "Python:  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"What we have in software: \\n Torch version:\", torch.__version__)\n",
        "print('Python: ', sys.version) # no switch case code"
      ],
      "metadata": {
        "id": "p_Ogx3y-_2Pr",
        "outputId": "62a9a654-42eb-431b-9441-1506d3ce6968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "p_Ogx3y-_2Pr",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What we have in software: \n",
            " Torch version: 1.12.0+cu113\n",
            "Python:  3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('What hardware the software see:')\n",
        "device = torch.device(\n",
        "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "print(device)\n",
        "num_of_gpus = torch.cuda.device_count()\n",
        "print(\"# of GPU\", num_of_gpus)"
      ],
      "metadata": {
        "id": "BVzrkN8kGgbY",
        "outputId": "d1f45a1a-1f9e-4682-f0a2-fc355b657bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BVzrkN8kGgbY",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What hardware the software see:\n",
            "cuda:0\n",
            "# of GPU 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yAvkOLvbGgep"
      },
      "id": "yAvkOLvbGgep",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Setup the problem"
      ],
      "metadata": {
        "id": "gL68ZWN-Gt-G"
      },
      "id": "gL68ZWN-Gt-G"
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for package debugging\n",
        "# for debug\n",
        "import importlib\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SuRSZhjbGgi1"
      },
      "id": "SuRSZhjbGgi1",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import PD_pLMProbXDiff.UtilityPack as UtilityPack\n",
        "# # run this when updating the package\n",
        "# importlib.reload(UtilityPack)"
      ],
      "metadata": {
        "id": "XZ-n_YDYG518"
      },
      "id": "XZ-n_YDYG518",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Global control key setup\n",
        "# ===============================================\n",
        "# Control keys:\n",
        "# This one will be directly modified for each task\n",
        "CKeys = dict(\n",
        "    #\n",
        "    Running_Type=2, # 1-local:engaging cluster, 2-supercould cluster, 3-google colab, 4-local ubuntu\n",
        "    #\n",
        "    # Working_Mode=1, # 1-training, 2-sampling for test\n",
        "    Working_Mode=2,\n",
        "    #\n",
        "    # IF_FirstRun=1,  # 1-1st run of training; otherwise, # of training run\n",
        "    IF_FirstRun=2,  # 1-1st run of training; otherwise, # of training run\n",
        "    #\n",
        "    # 1-SecStr-ModelB, 2-MD-ModelB, 3-SecStr-ModelA, 4-MD-ModelA\n",
        "    # 5-SecStr-ModelB-Embdding, 6-MD-ModelB, 7-SecStr-ModelA-pLM, 8-MD-ModelA\n",
        "    # 9-MD-Predictor-ModelB, 10-\n",
        "    # 11-MD-ModelB\n",
        "    Problem_ID=11, # 8, # 6,\n",
        "    #\n",
        "    # Debug=1, # 1-debug mode on; add more debug keys for different blocks\n",
        "    Debug=0, # 1-debug mode on; add more debug keys for different blocks\n",
        "    #\n",
        "    Debug_DataSet=1,\n",
        "    Debug_Model=1\n",
        "    #\n",
        "    # Debug=0\n",
        ")"
      ],
      "metadata": {
        "id": "-LIQ-g0TG55N"
      },
      "id": "-LIQ-g0TG55N",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if CKeys['Debug']==1:\n",
        "    # add some\n",
        "    CKeys['SlientRun']=0 # 1-save figure into files; 0-show figures\n",
        "    # detailed debug keys\n",
        "    # 1. for model dimension\n",
        "    CKeys['Debug_DataPack']=1\n",
        "    CKeys['Debug_ModelPack']=1\n",
        "    CKeys['Debug_TrainerPack']=1\n",
        "    # 2. for trainer part\n",
        "    CKeys['Debug_DataPack']=1\n",
        "    CKeys['Debug_ModelPack']=0\n",
        "    CKeys['Debug_TrainerPack']=3\n",
        "    # #\n",
        "    # CKeys['testratio']=0.15 # for small ForcPath problem\n",
        "    CKeys['testratio']=0.10 # for large ForcPath problem\n",
        "\n",
        "else:\n",
        "    # for real working run\n",
        "    CKeys['SlientRun']=1\n",
        "    #\n",
        "    CKeys['Debug_DataPack'] = 0\n",
        "    CKeys['Debug_ModelPack'] = 0\n",
        "    CKeys['Debug_TrainerPack'] = 0 # 2 # 1\n",
        "    # add some for training\n",
        "    CKeys['epochs'] = 4000-3250 # 1000 # 200\n",
        "    CKeys['print_loss_every_this_epochs']=50 # 5\n",
        "    CKeys['sample_every_this_epochs']=100 # 50 # 20\n",
        "    CKeys['save_model_every_this_epochs']=50 # 20\n",
        "    # #\n",
        "    # # add some for training\n",
        "    # CKeys['epochs'] = 2000 # 1000 # 200\n",
        "    # CKeys['print_loss_every_this_epochs']=20 # 5\n",
        "    # CKeys['sample_every_this_epochs']=50 # 50 # 20\n",
        "    # CKeys['save_model_every_this_epochs']=50 # 20\n",
        "    #\n",
        "    # CKeys['testratio']=0.15 # for small ForcPath problem\n",
        "    CKeys['testratio']=0.10 # for large ForcPath problem\n",
        "    #\n",
        "    # # add some for training\n",
        "    # CKeys['epochs'] = 4 # 1000 # 200\n",
        "    # CKeys['print_loss_every_this_epochs']=1 # 5\n",
        "    # CKeys['sample_every_this_epochs']=1 # 50 # 20\n",
        "    # CKeys['save_model_every_this_epochs']=2 # 20\n",
        "\n",
        "# for check\n",
        "print(json.dumps(CKeys, indent=4))\n"
      ],
      "metadata": {
        "id": "PKlYp37MG58N",
        "outputId": "10830da5-096c-4b42-ae65-7927c9a76611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PKlYp37MG58N",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Running_Type\": 2,\n",
            "    \"Working_Mode\": 2,\n",
            "    \"IF_FirstRun\": 2,\n",
            "    \"Problem_ID\": 11,\n",
            "    \"Debug\": 0,\n",
            "    \"Debug_DataSet\": 1,\n",
            "    \"Debug_Model\": 1,\n",
            "    \"SlientRun\": 1,\n",
            "    \"Debug_DataPack\": 0,\n",
            "    \"Debug_ModelPack\": 0,\n",
            "    \"Debug_TrainerPack\": 0,\n",
            "    \"epochs\": 750,\n",
            "    \"print_loss_every_this_epochs\": 50,\n",
            "    \"sample_every_this_epochs\": 100,\n",
            "    \"save_model_every_this_epochs\": 50,\n",
            "    \"testratio\": 0.1\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Problem type:\n",
        "print('Problem type: ', CKeys['Problem_ID'])\n",
        "print('Debug mode: ', CKeys['Debug'])\n",
        "print('Working mode: ', CKeys['Working_Mode'])\n"
      ],
      "metadata": {
        "id": "GO-CXtgKG5_E",
        "outputId": "34550d84-c37e-4fd5-c0e5-a7307be6567e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GO-CXtgKG5_E",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem type:  11\n",
            "Debug mode:  0\n",
            "Working mode:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===========================================================\n",
        "# Parameter key:\n",
        "# ===========================================================\n",
        "#\n",
        "PKeys = {}\n",
        "# define this one according the running environment\n",
        "# add more if needed\n",
        "# if CKeys['Running_Type']==1 or CKeys['Running_Type']==4:\n",
        "# if CKeys['Running_Type']==1:\n",
        "#\n",
        "#\n",
        "root_path = '/home/gridsan/bni/Test_ground/jupyter/1_git_project/sort_pdb_database_0/Local_Store/'\n",
        "if CKeys['Debug']==1:\n",
        "    # PKeys['prefix']='../Local_Store/For_16_0/'\n",
        "    # use the absolute path for transformability\n",
        "    PKeys['prefix']=root_path+'For_20_0/'\n",
        "\n",
        "if CKeys['Debug']!=1:\n",
        "    # PKeys['prefix']='../Local_Store/For_16_1/'\n",
        "    PKeys['prefix']=root_path+'For_20_1/'\n",
        "    PKeys['prefix']='/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/'\n",
        "    # add one for google colab\n",
        "    PKeys['prefix']=this_working_path # '/content/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/'\n",
        "\n",
        "# store the data pack after processing\n",
        "PKeys['pk_data_pack']=PKeys['prefix']+'data_pack.pickle'\n",
        "PKeys['pk_model_pack']=PKeys['prefix']+'model_pack.pickle'\n",
        "# PKeys[]"
      ],
      "metadata": {
        "id": "6pEl__x-IWsb"
      },
      "id": "6pEl__x-IWsb",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(CKeys['Running_Type'])\n",
        "# print(CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1)\n",
        "# print(PKeys['prefix'])"
      ],
      "metadata": {
        "id": "fDOFHpDTQgSf"
      },
      "id": "fDOFHpDTQgSf",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#\n",
        "# clean EVERYTHING in the dir if 1st\n",
        "#\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "    if os.path.exists(PKeys['prefix']):\n",
        "        cmd_line=f\"rm -r {PKeys['prefix']}\"\n",
        "        print(\"clean the slade...\")\n",
        "        print(f\"excute {cmd_line}\")\n",
        "        os.popen(cmd_line).read()\n",
        "        #\n",
        "    # create dir for working space\n",
        "    UtilityPack.create_path(PKeys['prefix'])"
      ],
      "metadata": {
        "id": "oPNALmfqQgW4"
      },
      "id": "oPNALmfqQgW4",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# prepare the csv files\n",
        "# ========================================================================\n",
        "\n",
        "if CKeys['Problem_ID']==1 or CKeys['Problem_ID']==3 \\\n",
        "or CKeys['Problem_ID']==5 or CKeys['Problem_ID']==7:\n",
        "#     SS_csv_file = PKeys['prefix']+'PROTEIN_Mar18_2022_SECSTR_ALL.csv'\n",
        "\n",
        "#     file_exists = os.path.exists(SS_csv_file)\n",
        "#     if not (file_exists):\n",
        "#         print('Downing the csv file...')\n",
        "#         print(os.popen(f\"wget https://www.dropbox.com/s/7o7s15w9qr6z76y/PROTEIN_Mar18_2022_SECSTR_ALL.csv -P {PKeys['prefix']}\").read())\n",
        "#         print('Done.')\n",
        "#     else:\n",
        "#         print(\"Already there\")\n",
        "    # +\n",
        "    # SS_csv_file = '/home/gridsan/bni/Test_ground/jupyter/1_git_project/sort_pdb_database_0/Local_Store/'+'PROTEIN_Mar18_2022_SECSTR_ALL.csv'\n",
        "    SS_csv_file = root_path+'Local_Store/'+'PROTEIN_Mar18_2022_SECSTR_ALL.csv'\n",
        "\n",
        "if CKeys['Problem_ID']==2 or CKeys['Problem_ID']==4 \\\n",
        "or CKeys['Problem_ID']==6 or CKeys['Problem_ID']==8 \\\n",
        "or CKeys['Problem_ID']==11:\n",
        "    # to be copied locally\n",
        "    # MD_smo_csv_file = '/home/gridsan/bni/Test_ground/jupyter/1_git_project/sort_pdb_database_0/Local_Store/For_1/ForTrain_recon_BSDB_LE_64_smd_disp_forc_df_smo.csv'\n",
        "    # first debug with LE_64, to be update into LE_128\n",
        "    MD_smo_csv_file = root_path+'For_1/ForTrain_recon_BSDB_LE_64_smd_disp_forc_df_smo.csv'\n",
        "    MD_smo_pk_file  = root_path+'For_1/ForTrain_recon_BSDB_LE_64_smd_disp_forc_df_smo_shared.pk'\n",
        "    # into LE_128\n",
        "    MD_smo_pk_file  = root_path+'For_1/ForTrain_recon_BSDB_LE_128_smd_disp_forc_df_smo.pk'"
      ],
      "metadata": {
        "id": "R9mD4zWwQgal"
      },
      "id": "R9mD4zWwQgal",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Handle the dataset"
      ],
      "metadata": {
        "id": "KExbXfH-Q8Qt"
      },
      "id": "KExbXfH-Q8Qt"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rw092iPzQgdS"
      },
      "id": "rw092iPzQgdS",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import PD_pLMProbXDiff.DataSetPack as DataSetPack\n",
        "# importlib.reload(DataSetPack)"
      ],
      "metadata": {
        "id": "PAhbQEOLQgfr"
      },
      "id": "PAhbQEOLQgfr",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('On Problem: ', CKeys['Problem_ID'])"
      ],
      "metadata": {
        "id": "8Akvo5jPQgnJ",
        "outputId": "d864cdf7-687b-490a-c947-fe443220a924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8Akvo5jPQgnJ",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On Problem:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(this_working_path)"
      ],
      "metadata": {
        "id": "OZW2AdRJIWwE",
        "outputId": "6ef89a5a-5480-485e-e28b-e305a02c70f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OZW2AdRJIWwE",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/working_results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on the sec_str csv file: SecStr\n",
        "# try to convey all para via one key\n",
        "# ====================================================\n",
        "# add some new keys for dataset\n",
        "# ====================================================\n",
        "# for data washing: only for 1st training cycle\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "\n",
        "    if CKeys['Problem_ID']==1:\n",
        "        pass\n",
        "        # print(\"1\")\n",
        "        # # +++++++++++++++++++++++++++++++++++++\n",
        "        # # SecStr as input seq\n",
        "        # DataKeys={}\n",
        "        # DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS/'\n",
        "        # # screening rules\n",
        "        # DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=128\n",
        "        # # X and Y processing\n",
        "        # DataKeys['Xnormfac']=9.\n",
        "        # DataKeys['ynormfac']=21.\n",
        "        # DataKeys['tokenizer_X']=None\n",
        "        # DataKeys['tokenizer_y']=None\n",
        "        # # + for AA embending using ESM\n",
        "        # DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # # deliver\n",
        "        # DataKeys['batch_size']=256\n",
        "        # DataKeys['batch_size']=200\n",
        "        # DataKeys['testset_ratio']=0.1\n",
        "        # DataKeys['maxdata']=99999999999999999\n",
        "        # # add the folder for Data part\n",
        "        # UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "    elif CKeys['Problem_ID']==2:\n",
        "        pass\n",
        "        print(\"2\")\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # MD record as the input seq\n",
        "#         #\n",
        "#         # try to convey all para via one key\n",
        "#         DataKeys={}\n",
        "#         # ======================================\n",
        "#         # keys for \"screen_dataset_MD\"\n",
        "#         DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "#         # add the folder\n",
        "#         UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "#         # screening rules\n",
        "#         DataKeys['min_AA_seq_len']=0\n",
        "#         DataKeys['max_AA_seq_len']=64\n",
        "#         DataKeys['max_Force_cap']=1000\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             file_path=MD_smo_csv_file,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "\n",
        "#         # save the dataframe\n",
        "#         pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "#         pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "#         # ======================================\n",
        "#         # keys for 2nd function\n",
        "#         DataKeys['X_Key']='sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "#         #\n",
        "#         DataKeys['tokenizer_X']=None # will not be used\n",
        "#         DataKeys['tokenizer_y']=None # to be created\n",
        "#         DataKeys['Xnormfac'] = np.max(protein_df['Max_Smo_Force'])\n",
        "#         print('Normalization factor for force: ', DataKeys['Xnormfac'])\n",
        "#         DataKeys['ynormfac']=21. # old force diffusion model 22.\n",
        "#         #\n",
        "#         DataKeys['batch_size']=256\n",
        "#         DataKeys['testset_ratio']=0.15\n",
        "#         DataKeys['maxdata']=99999999991000\n",
        "\n",
        "    elif CKeys['Problem_ID']==3:\n",
        "        pass\n",
        "        # print(\"3\")\n",
        "        # # +++++++++++++++++++++++++++++++++++++\n",
        "        # # SecStr as input seq\n",
        "        # DataKeys={}\n",
        "        # DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS_ModelA/'\n",
        "        # # screening rules\n",
        "        # DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64 # 128\n",
        "        # DataKeys['max_text_len']=8\n",
        "        # # X and Y processing\n",
        "        # DataKeys['Xnormfac']=1.\n",
        "        # DataKeys['ynormfac']=22. # 21.\n",
        "        # DataKeys['tokenizer_X']=None\n",
        "        # DataKeys['tokenizer_y']=None\n",
        "        # # deliver\n",
        "        # DataKeys['batch_size']=512\n",
        "        # # for debug purpose\n",
        "        # # DataKeys['batch_size']=1\n",
        "        # DataKeys['testset_ratio']= 0.1\n",
        "        # DataKeys['maxdata']=99999999999999999\n",
        "        # # add the folder for Data part\n",
        "        # UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "    elif CKeys['Problem_ID']==4:\n",
        "        pass\n",
        "\n",
        "#         print(\"4: input text condition, output sequence...\")\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # MD record as the input seq\n",
        "#         #\n",
        "#         # try to convey all para via one key\n",
        "#         DataKeys={}\n",
        "#         # ======================================\n",
        "#         # keys for \"screen_dataset_MD\"\n",
        "#         DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "#         # add the folder\n",
        "#         UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "#         # screening rules\n",
        "#         DataKeys['min_AA_seq_len']=0\n",
        "#         DataKeys['max_AA_seq_len']=64\n",
        "#         DataKeys['max_text_len']=2\n",
        "#         DataKeys['max_Force_cap']=1000\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             file_path=MD_smo_csv_file,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "\n",
        "#         # save the dataframe\n",
        "#         pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "#         pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "#         # ======================================\n",
        "#         # keys for 2nd function\n",
        "#         DataKeys['X_Key']=['Max_Smo_Force','Int_Smo_ForcPull'] # 'sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "#         #\n",
        "#         DataKeys['tokenizer_X']=None # will not be used\n",
        "#         DataKeys['tokenizer_y']=None # to be created\n",
        "#         #\n",
        "#         print('Normalization factor for force: ',\n",
        "#               np.max(protein_df['Max_Smo_Force']))\n",
        "#         print('Normalization factor for toughness: ',\n",
        "#               np.max(protein_df['Int_Smo_ForcPull']))\n",
        "#         #\n",
        "#         DataKeys['Xnormfac'] = np.array([\n",
        "#             np.max(protein_df['Max_Smo_Force']),\n",
        "#             np.max(protein_df['Int_Smo_ForcPull'])\n",
        "#         ])\n",
        "#         #\n",
        "#         DataKeys['ynormfac']=21. # old force diffusion model 22.\n",
        "#         #\n",
        "#         DataKeys['batch_size']=256\n",
        "#         DataKeys['testset_ratio']=0.15\n",
        "#         DataKeys['maxdata']=99999999991000\n",
        "\n",
        "    # /////////////////////////////////////////////////////////////\n",
        "    # try embedding\n",
        "    # \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "    elif CKeys['Problem_ID']==5:\n",
        "        pass\n",
        "\n",
        "#         print(\"5\")\n",
        "#         # +++++++++++++++++++++++++++++++++++++\n",
        "#         # SecStr as input seq\n",
        "#         DataKeys={}\n",
        "#         DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS/'\n",
        "#         # screening rules\n",
        "#         DataKeys['min_AA_seq_len']=0\n",
        "#         DataKeys['max_AA_seq_len']=128\n",
        "#         # X and Y processing\n",
        "#         DataKeys['Xnormfac']=9.\n",
        "#         DataKeys['ynormfac']=1 # for ESM # 21.\n",
        "#         DataKeys['tokenizer_X']=None\n",
        "#         DataKeys['tokenizer_y']=None\n",
        "#         # + for AA embending using ESM\n",
        "#         DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "#         # add for embedding space\n",
        "#         DataKeys['image_channels']=1280\n",
        "#         # deliver\n",
        "#         DataKeys['batch_size']=256\n",
        "#         DataKeys['batch_size']=256 # 0 # 200\n",
        "#         DataKeys['testset_ratio']=CKeys['testratio'] # 0.1\n",
        "#         DataKeys['maxdata']=99999999999999999\n",
        "\n",
        "#         # add the folder for Data part\n",
        "#         UtilityPack.create_path(DataKeys['data_dir'])\n",
        "    #\n",
        "    elif CKeys['Problem_ID']==6:\n",
        "        print(\"6, input MD ForcePath, output AA sequence\")\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # MD record as the tokenized input seq\n",
        "        #\n",
        "        # try to convey all para via one key\n",
        "        DataKeys={}\n",
        "        # ======================================\n",
        "        # keys for \"screen_dataset_MD\"\n",
        "        DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "        # add the folder\n",
        "        UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "        # screening rules\n",
        "        DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64\n",
        "        DataKeys['max_AA_seq_len']=128\n",
        "        DataKeys['max_Force_cap']=1000\n",
        "#         # ---------------------------------------\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             csv_file=MD_smo_csv_file,\n",
        "#             pk_file=None,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "            csv_file=None,\n",
        "            pk_file=MD_smo_pk_file,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "        # save the dataframe\n",
        "        pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "        pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "        # ======================================\n",
        "        # keys for 2nd function\n",
        "        DataKeys['X_Key']='sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "        #\n",
        "        DataKeys['tokenizer_X']=None # will not be used\n",
        "        DataKeys['tokenizer_y']=None # to be created\n",
        "        # think about this: update this one if necessary\n",
        "        # DataKeys['Xnormfac'] = np.max(protein_df['Max_Smo_Force'])\n",
        "        DataKeys['Xnormfac'] = 750.\n",
        "\n",
        "        print('Normalization factor for force: ', DataKeys['Xnormfac'])\n",
        "        DataKeys['ynormfac']=1. # not used as esm is used # 21. # old force diffusion model 22.\n",
        "        #\n",
        "        DataKeys['batch_size']=256\n",
        "        DataKeys['testset_ratio']=0.15\n",
        "        DataKeys['maxdata']=99999999991000\n",
        "        # ++ for pLM\n",
        "        # for AA embending using ESM\n",
        "        DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # add for embedding space\n",
        "        DataKeys['image_channels']=1280\n",
        "        #\n",
        "        DataKeys['ESM-2_Model']='esm2_t12_35M_UR50D'\n",
        "        DataKeys['image_channels']=480\n",
        "\n",
        "\n",
        "    elif CKeys['Problem_ID']==7:\n",
        "        pass\n",
        "        # print(\"7\")\n",
        "        # # +++++++++++++++++++++++++++++++++++++\n",
        "        # # SecStr text as input seq\n",
        "        # DataKeys={}\n",
        "        # DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_SS_ModelA/'\n",
        "        # # screening rules\n",
        "        # DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64 # 128\n",
        "        # DataKeys['max_text_len']=8\n",
        "        # # X and Y processing\n",
        "        # DataKeys['Xnormfac']=1.\n",
        "        # DataKeys['ynormfac']=1. # for ESM # 21. 22. # 21.\n",
        "        # DataKeys['tokenizer_X']=None\n",
        "        # DataKeys['tokenizer_y']=None\n",
        "        # # deliver\n",
        "        # DataKeys['batch_size']=512\n",
        "        # # for debug purpose\n",
        "        # # DataKeys['batch_size']=1\n",
        "        # DataKeys['testset_ratio']= CKeys['testratio'] # 0.1\n",
        "        # DataKeys['maxdata']=99999999999999999\n",
        "        # # + for AA embending using ESM\n",
        "        # DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # # add for embedding space\n",
        "        # DataKeys['image_channels']=1280\n",
        "        # #\n",
        "        # # add the folder for Data part\n",
        "        # UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "    elif CKeys['Problem_ID']==8:\n",
        "        #\n",
        "        print(\"8: input text condition, output sequence...\")\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # MD record as the input seq\n",
        "        #\n",
        "        # try to convey all para via one key\n",
        "        DataKeys={}\n",
        "        # ======================================\n",
        "        # keys for \"screen_dataset_MD\"\n",
        "        DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "        # add the folder\n",
        "        UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "        # screening rules\n",
        "        DataKeys['min_AA_seq_len']=0\n",
        "        DataKeys['max_AA_seq_len']=64\n",
        "        DataKeys['max_AA_seq_len']=128\n",
        "        DataKeys['max_text_len']=2\n",
        "        DataKeys['max_Force_cap']=1000\n",
        "#         # ---------------------------------------------------------\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             # # --\n",
        "#             # file_path=MD_smo_csv_file,\n",
        "#             # ++\n",
        "#             csv_file=MD_smo_csv_file,\n",
        "#             pk_file=None,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "            csv_file=None,\n",
        "            pk_file=MD_smo_pk_file,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "        # save the dataframe\n",
        "        pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "        pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "        # ======================================\n",
        "        # keys for 2nd function\n",
        "        DataKeys['X_Key']=['Max_Smo_Force','Int_Smo_ForcPull'] # 'sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "        #\n",
        "        DataKeys['tokenizer_X']=None # will not be used\n",
        "        DataKeys['tokenizer_y']=None # will not be used # to be created\n",
        "        #\n",
        "        print('Normalization factor for force: ',\n",
        "              np.max(protein_df['Max_Smo_Force']))\n",
        "        print('Normalization factor for toughness: ',\n",
        "              np.max(protein_df['Int_Smo_ForcPull']))\n",
        "        #\n",
        "        DataKeys['Xnormfac'] = np.array([\n",
        "            np.max(protein_df['Max_Smo_Force']),\n",
        "            np.max(protein_df['Int_Smo_ForcPull'])\n",
        "        ])\n",
        "        #\n",
        "        DataKeys['ynormfac']=1.0 # not used in esm # 21. # old force diffusion model 22.\n",
        "        #\n",
        "        DataKeys['batch_size']=256\n",
        "        DataKeys['testset_ratio']=CKeys['testratio'] # 0.15\n",
        "        DataKeys['maxdata']=99999999991000\n",
        "        # + for AA embending using ESM\n",
        "        DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # add for embedding space\n",
        "        DataKeys['image_channels']=1280\n",
        "\n",
        "    elif CKeys['Problem_ID']==11:\n",
        "        # copied from Problem_ID=6\n",
        "        #\n",
        "        print(\"11, input MD ForcePath, output AA sequence\")\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # MD record as the tokenized input seq\n",
        "        #\n",
        "        # try to convey all para via one key\n",
        "        DataKeys={}\n",
        "        # ======================================\n",
        "        # keys for \"screen_dataset_MD\"\n",
        "        DataKeys['data_dir']=PKeys['prefix']+'0_dataprocess_MD/'\n",
        "        # add the folder\n",
        "        UtilityPack.create_path(DataKeys['data_dir'])\n",
        "\n",
        "        # screening rules\n",
        "        DataKeys['min_AA_seq_len']=0\n",
        "        # DataKeys['max_AA_seq_len']=64\n",
        "        DataKeys['max_AA_seq_len']=128\n",
        "        DataKeys['max_Force_cap']=1000\n",
        "#         # ---------------------------------------\n",
        "#         # special ones\n",
        "#         # change text arr into np arr\n",
        "#         DataKeys['arr_key']=[\n",
        "#             'posi_data','pull_data','forc_data',\n",
        "#             'gap_data','normalized_gap_data',\n",
        "#             'pull_gap_data', 'normalized_pull_gap_data',\n",
        "#             'sample_NormPullGap_data','sample_FORCEpN_data']\n",
        "\n",
        "#         df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "#             csv_file=MD_smo_csv_file,\n",
        "#             pk_file=None,\n",
        "#             PKeys=DataKeys, # to be updated\n",
        "#             CKeys=CKeys,\n",
        "#         )\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        df_raw, protein_df = DataSetPack.screen_dataset_MD(\n",
        "            csv_file=None,\n",
        "            pk_file=MD_smo_pk_file,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "        # save the dataframe\n",
        "        pd.to_pickle(protein_df, DataKeys['data_dir']+'protein_df.pk')\n",
        "        pd.to_pickle(df_raw, DataKeys['data_dir']+'df_raw.pk')\n",
        "\n",
        "        # ======================================\n",
        "        # keys for 2nd function\n",
        "        DataKeys['X_Key']='sample_FORCEpN_data' # or 'Max_Smo_Force'\n",
        "        #\n",
        "        DataKeys['tokenizer_X']=None # will not be used\n",
        "        DataKeys['tokenizer_y']=None # to be created\n",
        "        # think about this: update this one if necessary\n",
        "        # DataKeys['Xnormfac'] = np.max(protein_df['Max_Smo_Force'])\n",
        "        DataKeys['Xnormfac'] = 750.\n",
        "\n",
        "        print('Normalization factor for force: ', DataKeys['Xnormfac'])\n",
        "        DataKeys['ynormfac']=1. # not used as esm is used # 21. # old force diffusion model 22.\n",
        "        #\n",
        "        DataKeys['batch_size']=256\n",
        "        DataKeys['testset_ratio']=0.15\n",
        "        DataKeys['maxdata']=99999999991000\n",
        "        # ++ for pLM\n",
        "        # for AA embending using ESM\n",
        "        DataKeys['ESM-2_Model']='esm2_t33_650M_UR50D'\n",
        "        # add for embedding space\n",
        "        # DataKeys['image_channels']=1280\n",
        "        #\n",
        "        # DataKeys['ESM-2_Model']='esm2_t12_35M_UR50D'\n",
        "        # # DataKeys['image_channels']=480\n",
        "        #\n",
        "        # DataKeys['ESM-2_Model']='esm2_t36_3B_UR50D'\n",
        "        # DataKeys['image_channels']=2560\n",
        "        #\n",
        "        DataKeys['ESM-2_Model']='esm2_t30_150M_UR50D'\n",
        "        # DataKeys['image_channels']=640\n",
        "\n",
        "        # only use the probability part\n",
        "        DataKeys['image_channels']=33\n",
        "\n",
        "    else:\n",
        "        print('No Problem Type found...')\n",
        "# else:\n",
        "#     # load back if there is anything generated in the 1st run\n",
        "#     if CKeys['Problem_ID']==2 or CKeys['Problem_ID']==6:\n",
        "#         protein_df = pd.read_pickle(DataKeys['data_dir']+'protein_df.pk')\n",
        "#         df_raw = pd.read_pickle(DataKeys['data_dir']+'df_raw.pk')"
      ],
      "metadata": {
        "id": "msXLgSXqIWzi"
      },
      "id": "msXLgSXqIWzi",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(CKeys)"
      ],
      "metadata": {
        "id": "2GhYVnk2IW3l",
        "outputId": "f7fbec29-b264-450c-eaaf-361ce1da1e35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2GhYVnk2IW3l",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Running_Type': 2, 'Working_Mode': 2, 'IF_FirstRun': 2, 'Problem_ID': 11, 'Debug': 0, 'Debug_DataSet': 1, 'Debug_Model': 1, 'SlientRun': 1, 'Debug_DataPack': 0, 'Debug_ModelPack': 0, 'Debug_TrainerPack': 0, 'epochs': 750, 'print_loss_every_this_epochs': 50, 'sample_every_this_epochs': 100, 'save_model_every_this_epochs': 50, 'testratio': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ====================================================\n",
        "# convert into datasets\n",
        "# ====================================================\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "    if CKeys['Problem_ID']==1:\n",
        "        pass\n",
        "        # train_loader, \\\n",
        "        # train_loader_noshuffle, \\\n",
        "        # test_loader, \\\n",
        "        # tokenizer_y, tokenizer_X = DataSetPack.load_data_set_SS_InSeqToOuSeq(\n",
        "        #     file_path=SS_csv_file,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==2:\n",
        "        pass\n",
        "        # train_loader, train_loader_noshuffle, \\\n",
        "        # test_loader, tokenizer_y, tokenizer_X = DataSetPack.load_data_set_from_df_SMD(\n",
        "        #     protein_df,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==3:\n",
        "        pass\n",
        "        # train_loader, train_loader_noshuffle, \\\n",
        "        # test_loader,tokenizer_y, tokenizer_X = DataSetPack.load_data_set_seq2seq_SecStr_ModelA (\n",
        "        #     file_path=SS_csv_file, # 'PROTEIN_Mar18_2022_SECSTR_ALL.csv',\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==4:\n",
        "        pass\n",
        "        # train_loader, train_loader_noshuffle, \\\n",
        "        # test_loader,tokenizer_y, tokenizer_X = DataSetPack.load_data_set_text2seq_MD_ModelA (\n",
        "        #     protein_df,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    # ///////////////////////////////////////////////////////////////\n",
        "    #  add embedding cases\n",
        "    # \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
        "    elif CKeys['Problem_ID']==5:\n",
        "        pass\n",
        "        # train_loader, \\\n",
        "        # train_loader_noshuffle, \\\n",
        "        # test_loader, \\\n",
        "        # tokenizer_y, \\\n",
        "        # tokenizer_X = DataSetPack.load_data_set_SS_InSeqToOuSeq_pLM(\n",
        "        #     file_path=SS_csv_file,\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "        # # this will triger the following downloading\n",
        "        # # Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /home/gridsan/bni/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
        "        # # excute the following if without internet on the node\n",
        "        # # 1 $ wget https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt -O  /home/gridsan/bni/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n",
        "        # # 2 $ wget https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt -O /home/gridsan/bni/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n",
        "\n",
        "    # add if needed\n",
        "    elif CKeys['Problem_ID']==6:\n",
        "\n",
        "        train_loader, \\\n",
        "        train_loader_noshuffle, \\\n",
        "        test_loader, \\\n",
        "        tokenizer_y, \\\n",
        "        tokenizer_X = DataSetPack.load_data_set_from_df_SMD_pLM(\n",
        "                protein_df,\n",
        "                PKeys=DataKeys, # to be updated\n",
        "                CKeys=CKeys,\n",
        "            )\n",
        "\n",
        "    elif CKeys['Problem_ID']==7:\n",
        "        pass\n",
        "        # train_loader, \\\n",
        "        # train_loader_noshuffle, \\\n",
        "        # test_loader, \\\n",
        "        # tokenizer_y, \\\n",
        "        # tokenizer_X = DataSetPack.load_data_set_seq2seq_SecStr_ModelA_pLM (\n",
        "        #     file_path=SS_csv_file, # 'PROTEIN_Mar18_2022_SECSTR_ALL.csv',\n",
        "        #     PKeys=DataKeys, # to be updated\n",
        "        #     CKeys=CKeys,\n",
        "        # )\n",
        "\n",
        "    elif CKeys['Problem_ID']==8:\n",
        "\n",
        "        train_loader, \\\n",
        "        train_loader_noshuffle, \\\n",
        "        test_loader,\\\n",
        "        tokenizer_y, \\\n",
        "        tokenizer_X = DataSetPack.load_data_set_text2seq_MD_ModelA_pLM (\n",
        "            protein_df,\n",
        "            PKeys=DataKeys, # to be updated\n",
        "            CKeys=CKeys,\n",
        "        )\n",
        "\n",
        "    elif CKeys['Problem_ID']==11:\n",
        "\n",
        "        train_loader, \\\n",
        "        train_loader_noshuffle, \\\n",
        "        test_loader, \\\n",
        "        tokenizer_y, \\\n",
        "        tokenizer_X = DataSetPack.load_data_set_from_df_SMD_pLM(\n",
        "                protein_df,\n",
        "                PKeys=DataKeys, # to be updated\n",
        "                CKeys=CKeys,\n",
        "            )\n",
        "\n",
        "    elif CKeys['Problem_ID']==12:\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        print('No Problem Type found...')\n",
        "\n",
        "    print(\"==========================================\")\n",
        "    print(\"Save the datasets ...\")\n",
        "    print(\"==========================================\")\n",
        "    # save the dataset for for the 1st run\n",
        "    data_pack = {}\n",
        "    data_pack['train_loader']=train_loader\n",
        "    data_pack['train_loader_noshuffle']=train_loader_noshuffle\n",
        "    data_pack['test_loader']=test_loader\n",
        "    data_pack['tokenizer_X']=tokenizer_X\n",
        "    data_pack['tokenizer_y']=tokenizer_y\n",
        "    # keys\n",
        "    data_pack['DataKeys']=DataKeys\n",
        "    # data_pack['CKeys']=CKeys\n",
        "    data_pack['PKeys']=PKeys\n",
        "    with open(PKeys['pk_data_pack'], 'wb') as handle:\n",
        "        pickle.dump(data_pack, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "else: # work both for training and testing\n",
        "\n",
        "    print('This is not the first run')\n",
        "    print('Load back in the data packages...')\n",
        "    with open(PKeys['pk_data_pack'], 'rb') as handle:\n",
        "        data_pack = pickle.load(handle)\n",
        "    # deliver the results\n",
        "    train_loader=data_pack['train_loader']\n",
        "    train_loader_noshuffle=data_pack['train_loader_noshuffle']\n",
        "    test_loader=data_pack['test_loader']\n",
        "    tokenizer_X=data_pack['tokenizer_X']\n",
        "    tokenizer_y=data_pack['tokenizer_y']\n",
        "    # keys (create or update)\n",
        "    DataKeys=data_pack['DataKeys']\n",
        "    # ++ for colab, need to update the path part\n",
        "    DataKeys['data_dir']=this_working_path+'0_dataprocess_MD/'\n",
        "    # ++\n",
        "    # CKeys=data_pack['CKeys']\n",
        "    PKeys=data_pack['PKeys']\n",
        "    # ++ for colab, need to update the path part\n",
        "    PKeys['prefix']=this_working_path\n",
        "    PKeys['pk_data_pack']=this_working_path+'data_pack.pickle'\n",
        "    PKeys['pk_model_pack']=this_working_path+'model_pack.pickle'\n",
        "    # ++\n",
        "    # add some for specific problem\n",
        "    if CKeys['Problem_ID']==2 or CKeys['Problem_ID']==6 \\\n",
        "    or CKeys['Problem_ID']==11:\n",
        "        pass\n",
        "        # skip\n",
        "        # protein_df = pd.read_pickle(DataKeys['data_dir']+'protein_df.pk')\n",
        "        # df_raw = pd.read_pickle(DataKeys['data_dir']+'df_raw.pk')\n",
        "    print('Done.')\n",
        "\n"
      ],
      "metadata": {
        "id": "ExgiEEdLRZCP",
        "outputId": "0f6e55c3-a678-4ba3-9462-78f8dae0bb3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ExgiEEdLRZCP",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is not the first run\n",
            "Load back in the data packages...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DataKeys)\n",
        "print(this_working_path)"
      ],
      "metadata": {
        "id": "8terjIncfRAC",
        "outputId": "461643da-224e-4ead-ddcc-8555c4d01790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8terjIncfRAC",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data_dir': '/content/working_results/0_dataprocess_MD/', 'min_AA_seq_len': 0, 'max_AA_seq_len': 128, 'max_Force_cap': 1000, 'X_Key': 'sample_FORCEpN_data', 'tokenizer_X': None, 'tokenizer_y': None, 'Xnormfac': 750.0, 'ynormfac': 1.0, 'batch_size': 256, 'testset_ratio': 0.15, 'maxdata': 99999999991000, 'ESM-2_Model': 'esm2_t30_150M_UR50D', 'image_channels': 33}\n",
            "/content/working_results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Prepare for de novo conditions"
      ],
      "metadata": {
        "id": "mxnaebDgjRU9"
      },
      "id": "mxnaebDgjRU9"
    },
    {
      "cell_type": "code",
      "source": [
        "# add a block to handle the De Novo force shape\n",
        "# idea: pick to to mix them\n",
        "# if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1 and CKeys['Problem_ID']==2:\n",
        "if CKeys['Working_Mode']==1 and CKeys['Problem_ID']==2:\n",
        "    print( protein_df.keys() )\n",
        "    # pick the Max_Smo_Force max and min\n",
        "    pick_id_0 = protein_df['Max_Smo_Force'].idxmax()\n",
        "    pick_id_1 = protein_df['Max_Smo_Force'].idxmin()\n",
        "    print(protein_df['Max_Smo_Force'].idxmax())\n",
        "    print(protein_df['Max_Smo_Force'].idxmin())\n",
        "\n",
        "    fig = plt.figure(figsize=(24,16),dpi=200)\n",
        "    fig, ax0 = plt.subplots()\n",
        "    for ii in [pick_id_0, pick_id_1]:\n",
        "        ax0.plot(\n",
        "                protein_df['sample_NormPullGap_data'][ii],\n",
        "                protein_df['sample_FORCE_data'][ii],\n",
        "                alpha=0.1,\n",
        "                # color=\"green\",label='simplified data',\n",
        "                # linestyle='None',marker='^'\n",
        "            )\n",
        "        ax0.scatter(\n",
        "                protein_df['NPullGap_for_MaxSmoF'][ii],\n",
        "                protein_df['Max_Smo_Force'][ii],\n",
        "            )\n",
        "    plt.xlabel('Normalized distance btw pulling ends')\n",
        "    plt.ylabel('Force (pF)')\n",
        "    outname = DataKeys['data_dir']+'CSV_6_PickMaxMinF_SMD_sim_Dist.jpg'\n",
        "    if CKeys['SlientRun'] ==1:\n",
        "        plt.savefig(outname, dpi=200)\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "elif CKeys['Working_Mode']==1 and CKeys['Problem_ID']==5:\n",
        "    #\n",
        "    pass\n",
        "    #\n",
        "elif CKeys['Working_Mode']==1 and CKeys['Problem_ID']==6:\n",
        "    print( protein_df.keys() )\n",
        "    # pick the Max_Smo_Force max and min\n",
        "    pick_id_0 = protein_df['Max_Smo_Force'].idxmax()\n",
        "    pick_id_1 = protein_df['Max_Smo_Force'].idxmin()\n",
        "    # print(protein_df['Max_Smo_Force'].idxmax())\n",
        "    # print(protein_df['Max_Smo_Force'].idxmin())\n",
        "    print(f\"Max Peak Force id: {pick_id_0}\")\n",
        "    print(f\"Min Peak Force id: {pick_id_1}\")\n",
        "\n",
        "    fig = plt.figure(figsize=(24,16),dpi=200)\n",
        "    fig, ax0 = plt.subplots()\n",
        "    for ii in [pick_id_0, pick_id_1]:\n",
        "        ax0.plot(\n",
        "                protein_df['sample_NormPullGap_data'][ii],\n",
        "                protein_df['sample_FORCE_data'][ii],\n",
        "                alpha=0.1,\n",
        "                # color=\"green\",label='simplified data',\n",
        "                # linestyle='None',marker='^'\n",
        "            )\n",
        "        ax0.scatter(\n",
        "                protein_df['NPullGap_for_MaxSmoF'][ii],\n",
        "                protein_df['Max_Smo_Force'][ii],\n",
        "            )\n",
        "    plt.xlabel('Normalized distance btw pulling ends')\n",
        "    plt.ylabel('Force (pF)')\n",
        "    outname = DataKeys['data_dir']+'CSV_6_PickMaxMinF_SMD_sim_Dist.jpg'\n",
        "    if CKeys['SlientRun'] ==1:\n",
        "        plt.savefig(outname, dpi=200)\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "elif CKeys['Working_Mode']==1 and CKeys['Problem_ID']==11:\n",
        "    # skipped\n",
        "    pass\n",
        "    # print( protein_df.keys() )\n",
        "    # # pick the Max_Smo_Force max and min\n",
        "    # pick_id_0 = protein_df['Max_Smo_Force'].idxmax()\n",
        "    # pick_id_1 = protein_df['Max_Smo_Force'].idxmin()\n",
        "    # # print(protein_df['Max_Smo_Force'].idxmax())\n",
        "    # # print(protein_df['Max_Smo_Force'].idxmin())\n",
        "    # print(f\"Max Peak Force id: {pick_id_0}\")\n",
        "    # print(f\"Min Peak Force id: {pick_id_1}\")\n",
        "\n",
        "    # fig = plt.figure(figsize=(24,16),dpi=200)\n",
        "    # fig, ax0 = plt.subplots()\n",
        "    # for ii in [pick_id_0, pick_id_1]:\n",
        "    #     ax0.plot(\n",
        "    #             protein_df['sample_NormPullGap_data'][ii],\n",
        "    #             protein_df['sample_FORCE_data'][ii],\n",
        "    #             alpha=0.1,\n",
        "    #             # color=\"green\",label='simplified data',\n",
        "    #             # linestyle='None',marker='^'\n",
        "    #         )\n",
        "    #     ax0.scatter(\n",
        "    #             protein_df['NPullGap_for_MaxSmoF'][ii],\n",
        "    #             protein_df['Max_Smo_Force'][ii],\n",
        "    #         )\n",
        "    # plt.xlabel('Normalized distance btw pulling ends')\n",
        "    # plt.ylabel('Force (pF)')\n",
        "    # outname = DataKeys['data_dir']+'CSV_6_PickMaxMinF_SMD_sim_Dist.jpg'\n",
        "    # if CKeys['SlientRun'] ==1:\n",
        "    #     plt.savefig(outname, dpi=200)\n",
        "    # else:\n",
        "    #     plt.show()\n",
        "    # plt.close()"
      ],
      "metadata": {
        "id": "TwXmEpWhjWcI"
      },
      "id": "TwXmEpWhjWcI",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a quick check\n",
        "print('tokenizer_X: ', tokenizer_X)\n",
        "print('tokenizer_y: ', tokenizer_y)\n",
        "print('Xnormfac: ', DataKeys['Xnormfac'])\n",
        "print('ynormfac: ', DataKeys['ynormfac'])\n"
      ],
      "metadata": {
        "id": "Fj-mgBvijs36",
        "outputId": "f6a6d9d0-a91e-4ce0-c82b-76b82bb8cc2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Fj-mgBvijs36",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_X:  None\n",
            "tokenizer_y:  None\n",
            "Xnormfac:  750.0\n",
            "ynormfac:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if CKeys['Debug']==1:\n",
        "\n",
        "    if CKeys['Debug_DataPack']==1:\n",
        "        # uppack to check\n",
        "        print('Len of train loader:', len(train_loader))\n",
        "        print('Len of test loader:', len(test_loader))\n",
        "        ii=-1\n",
        "        # for item in train_loader:\n",
        "        for item in test_loader:\n",
        "            ii += 1\n",
        "            if ii<1:\n",
        "                print('Len of 1st batch item: ', len(item))\n",
        "                this_item = item\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        # on mini-batch\n",
        "        print('Batch size: ', DataKeys['batch_size'])\n",
        "        print('Seq len: ', DataKeys['max_AA_seq_len'])\n",
        "        print('X.dim: ', this_item[0].shape) # Condition: (Batch, Condi)\n",
        "        print('y_data.dim: ', this_item[1].shape) # AASequenc: (Batch, AACode)\n",
        "        #\n",
        "        # # print(this_item[0][0,:]*DataKeys['Xnormfac'])\n",
        "        # # print(this_item[1][0,:]*DataKeys['ynormfac'])\n",
        "        print(this_item[0][0,:])\n",
        "        # print(torch.FloatTensor(DataKeys['Xnormfac']))\n",
        "        # print(this_item[0][0,:]*torch.FloatTensor(DataKeys['Xnormfac']))\n"
      ],
      "metadata": {
        "id": "_skH2fNyjs69"
      },
      "id": "_skH2fNyjs69",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CKeys['Debug']==1:\n",
        "\n",
        "    if CKeys['Debug_DataPack']==1:\n",
        "\n",
        "        if CKeys['Problem_ID']==8 or CKeys['Problem_ID']==7:\n",
        "\n",
        "            print (this_item[0][0,:])\n",
        "            print (this_item[0][0,:]*DataKeys['Xnormfac'])"
      ],
      "metadata": {
        "id": "whOEVBtajs-J"
      },
      "id": "whOEVBtajs-J",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Model building"
      ],
      "metadata": {
        "id": "M7ehoWibj6Sa"
      },
      "id": "M7ehoWibj6Sa"
    },
    {
      "cell_type": "code",
      "source": [
        "# import PD_pLMProbXDiff.ModelPack as ModelPack\n",
        "# #\n",
        "# import PD_pLMProbXDiff.TrainerPack as TrainerPack\n",
        "# #\n",
        "# importlib.reload(ModelPack)\n",
        "# importlib.reload(TrainerPack)"
      ],
      "metadata": {
        "id": "4lE_XT0bjtAV"
      },
      "id": "4lE_XT0bjtAV",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if CKeys['Working_Mode']==1 and CKeys['IF_FirstRun']==1:\n",
        "    # this is a trining mode\n",
        "    if CKeys['Problem_ID']==1:\n",
        "        pass\n",
        "#         # this is 1st run....\n",
        "#         # +++++++++++++++++++++++++++++++++++++\n",
        "#         # SecStr as input seq\n",
        "#         ModelKeys={}\n",
        "#         # storage\n",
        "#         ModelKeys['model_dir']=PKey['prefix']+'1_model_SS/'\n",
        "#         # create the folder\n",
        "#         print(\"Creating the model dir...\")\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "#         # secondary folders\n",
        "#         ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "#         ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "#         # for UNet\n",
        "#         ModelKeys['dim']=256    # dim for UNet\n",
        "#         ModelKeys['text_embed_dim'] = 512\n",
        "#         ModelKeys['embed_dim_position']=32\n",
        "#         ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "#         ModelKeys['cond_dim'] = 512\n",
        "#         ModelKeys['cond_images_channels']=1\n",
        "#         ModelKeys['max_text_len']=DataKeys['max_AA_seq_len'] # this is about text condi\n",
        "#         # for Imagen\n",
        "#         ModelKeys['pred_dim']=1 # for sequence, =1\n",
        "#         ModelKeys['diff_timesteps']=(96,)\n",
        "#         ModelKeys['loss_type']=0 # MSE\n",
        "#         ModelKeys['elucidated']=True #\n",
        "#         ModelKeys['padding_idx']=0\n",
        "#         ModelKeys['max_length']=DataKeys['max_AA_seq_len'] # max seq len\n",
        "#         ModelKeys['device']=device\n",
        "#         #\n",
        "#         # buckets for Model building\n",
        "#         ModelKeys['UNet']={}\n",
        "#         ModelKeys['Imagen']={}\n",
        "#         # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#         write_PK_UNet=dict()\n",
        "#         write_PK_UNet['dim']=ModelKeys['dim'] # 256\n",
        "#         write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim'] # 512+32\n",
        "#         write_PK_UNet['num_resnet_blocks']=1\n",
        "#         write_PK_UNet['cond_dim']=ModelKeys['cond_dim']  #this is where text embeddings are projected to...\n",
        "#         # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "#         # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "#         # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "#         # write_PK_UNet['out_dim']=None\n",
        "#         write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "#         write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_UNet['channels']=1\n",
        "#         write_PK_UNet['channels_out']=1\n",
        "\n",
        "#         write_PK_UNet['attn_dim_head']=64\n",
        "#         write_PK_UNet['attn_heads']=8\n",
        "#         write_PK_UNet['ff_mult']=2.\n",
        "#         write_PK_UNet['lowres_cond']=False  # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "#         write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['layer_attns_depth']=1\n",
        "#         write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "#         write_PK_UNet['attend_at_middle']=True          # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "#         write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['use_linear_attn']=False\n",
        "#         write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "#         write_PK_UNet['cond_on_text']=True\n",
        "#         write_PK_UNet['max_text_len']=ModelKeys['max_text_len'] # need to check this one\n",
        "#         # ModelKeys['UNet']['init_dim']=None\n",
        "#         write_PK_UNet['resnet_groups']=8\n",
        "#         write_PK_UNet['init_conv_kernel_size']=7 # kernel size of initial conv, if not using cross embed\n",
        "#         write_PK_UNet['init_cross_embed']=False  #TODO - fix ouput size calcs for conv1d\n",
        "#         write_PK_UNet['init_cross_embed_kernel_sizes']=(3, 7, 15)\n",
        "#         write_PK_UNet['cross_embed_downsample']=False\n",
        "#         write_PK_UNet['cross_embed_downsample_kernel_sizes']=(2, 4)\n",
        "\n",
        "#         write_PK_UNet['attn_pool_text']=True\n",
        "#         write_PK_UNet['attn_pool_num_latents']=32 #perceiver model latents\n",
        "#         write_PK_UNet['dropout']=0.\n",
        "#         write_PK_UNet['memory_efficient']=False\n",
        "#         write_PK_UNet['init_conv_to_final_conv_residual']=False\n",
        "\n",
        "#         write_PK_UNet['use_global_context_attn']=True\n",
        "#         write_PK_UNet['scale_skip_connection']=True\n",
        "#         write_PK_UNet['final_resnet_block']=True\n",
        "#         write_PK_UNet['final_conv_kernel_size']=3\n",
        "\n",
        "#         write_PK_UNet['cosine_sim_attn']=True\n",
        "#         write_PK_UNet['self_cond']=False\n",
        "#         write_PK_UNet['combine_upsample_fmaps']=True    # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "#         write_PK_UNet['pixel_shuffle_upsample']=False   # may address checkboard artifacts\n",
        "#         # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "#         ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "#         # +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#         # beyond UNet, for Whole model, all keys\n",
        "#         # \"None\" means defult value on function definition\n",
        "#         write_PK_Imagen=dict()\n",
        "\n",
        "#         write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "#         write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "#         write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "#         write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "#         write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "#         # write_PK_Imagen['cond_dim']=None # use default\n",
        "#         # write_PK_Imagen['text_embed_dim']=None\n",
        "#         # write_PK_Imagen['input_tokens']=None\n",
        "#         # write_PK_Imagen['sequence_embed']=None\n",
        "#         # write_PK_Imagen['embed_dim_position']=None\n",
        "#         write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "#         write_PK_Imagen['cond_images_channels']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "#         write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "#         # extend it to a full key\n",
        "#         ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "\n",
        "    elif CKeys['Problem_ID']==2:\n",
        "        pass\n",
        "#         # this is 1st run....\n",
        "#         # +++++++++++++++++++++++++++++++++++++\n",
        "#         # SecStr as input seq\n",
        "#         ModelKeys={}\n",
        "#         # storage\n",
        "#         ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "#         # create the folder\n",
        "#         print(\"Creating the model dir...\")\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "#         # secondary folders\n",
        "#         ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "#         ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "#         # for UNet\n",
        "#         ModelKeys['dim']=256    # dim for UNet\n",
        "#         ModelKeys['text_embed_dim'] = 512\n",
        "#         ModelKeys['embed_dim_position']=32\n",
        "#         ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "#         ModelKeys['cond_dim'] = 512\n",
        "#         ModelKeys['cond_images_channels']=1\n",
        "#         ModelKeys['max_text_len']=DataKeys['max_AA_seq_len'] # this is about text condi\n",
        "#         # for Imagen\n",
        "#         ModelKeys['pred_dim']=1 # for sequence, =1\n",
        "#         ModelKeys['diff_timesteps']=(96,)\n",
        "#         ModelKeys['loss_type']=0 # MSE\n",
        "#         ModelKeys['elucidated']=True #\n",
        "#         ModelKeys['padding_idx']=0\n",
        "#         ModelKeys['max_length']=DataKeys['max_AA_seq_len'] # max seq len\n",
        "#         ModelKeys['device']=device\n",
        "#         #\n",
        "#         # buckets for Model building\n",
        "#         ModelKeys['UNet']={}\n",
        "#         ModelKeys['Imagen']={}\n",
        "#         # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#         write_PK_UNet=dict()\n",
        "#         write_PK_UNet['dim']=ModelKeys['dim'] # 256\n",
        "#         write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim'] # 512+32\n",
        "#         write_PK_UNet['num_resnet_blocks']=1\n",
        "#         write_PK_UNet['cond_dim']=ModelKeys['cond_dim']  #this is where text embeddings are projected to...\n",
        "#         # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "#         # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "#         # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "#         # write_PK_UNet['out_dim']=None\n",
        "#         write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "#         write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_UNet['channels']=1\n",
        "#         write_PK_UNet['channels_out']=1\n",
        "\n",
        "#         write_PK_UNet['attn_dim_head']=64\n",
        "#         write_PK_UNet['attn_heads']=8\n",
        "#         write_PK_UNet['ff_mult']=2.\n",
        "#         write_PK_UNet['lowres_cond']=False  # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "#         write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['layer_attns_depth']=1\n",
        "#         write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "#         write_PK_UNet['attend_at_middle']=True          # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "#         write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['use_linear_attn']=False\n",
        "#         write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "#         write_PK_UNet['cond_on_text']=True\n",
        "#         write_PK_UNet['max_text_len']=ModelKeys['max_text_len'] # need to check this one\n",
        "#         # ModelKeys['UNet']['init_dim']=None\n",
        "#         write_PK_UNet['resnet_groups']=8\n",
        "#         write_PK_UNet['init_conv_kernel_size']=7 # kernel size of initial conv, if not using cross embed\n",
        "#         write_PK_UNet['init_cross_embed']=False  #TODO - fix ouput size calcs for conv1d\n",
        "#         write_PK_UNet['init_cross_embed_kernel_sizes']=(3, 7, 15)\n",
        "#         write_PK_UNet['cross_embed_downsample']=False\n",
        "#         write_PK_UNet['cross_embed_downsample_kernel_sizes']=(2, 4)\n",
        "\n",
        "#         write_PK_UNet['attn_pool_text']=True\n",
        "#         write_PK_UNet['attn_pool_num_latents']=32 #perceiver model latents\n",
        "#         write_PK_UNet['dropout']=0.\n",
        "#         write_PK_UNet['memory_efficient']=False\n",
        "#         write_PK_UNet['init_conv_to_final_conv_residual']=False\n",
        "\n",
        "#         write_PK_UNet['use_global_context_attn']=True\n",
        "#         write_PK_UNet['scale_skip_connection']=True\n",
        "#         write_PK_UNet['final_resnet_block']=True\n",
        "#         write_PK_UNet['final_conv_kernel_size']=3\n",
        "\n",
        "#         write_PK_UNet['cosine_sim_attn']=True\n",
        "#         write_PK_UNet['self_cond']=False\n",
        "#         write_PK_UNet['combine_upsample_fmaps']=True    # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "#         write_PK_UNet['pixel_shuffle_upsample']=False   # may address checkboard artifacts\n",
        "#         # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "#         ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "#         # +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#         # beyond UNet, for Whole model, all keys\n",
        "#         # \"None\" means defult value on function definition\n",
        "#         write_PK_Imagen=dict()\n",
        "\n",
        "#         write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "#         write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "#         write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "#         write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "#         write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "#         # write_PK_Imagen['cond_dim']=None # use default\n",
        "#         # write_PK_Imagen['text_embed_dim']=None\n",
        "#         # write_PK_Imagen['input_tokens']=None\n",
        "#         # write_PK_Imagen['sequence_embed']=None\n",
        "#         # write_PK_Imagen['embed_dim_position']=None\n",
        "#         write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "#         write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "#         write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "#         # extend it to a full key\n",
        "#         ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "#         # to be defined\n",
        "\n",
        "    elif CKeys['Problem_ID']==3:\n",
        "        pass\n",
        "\n",
        "#         ModelKeys={}\n",
        "#         # storage\n",
        "#         ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "#         # create the folder\n",
        "#         print(\"Creating the model dir...\")\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "#         # secondary folders\n",
        "#         ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "#         ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "#         # for UNet\n",
        "#         ModelKeys['dim']=768    # dim for UNet\n",
        "#         # for debug\n",
        "#         ModelKeys['dim']=256    # dim for UNet: GPU stat: 256:16093MiB;\n",
        "#         #\n",
        "#         ModelKeys['text_embed_dim']=512-128\n",
        "#         ModelKeys['embed_dim_position']=128\n",
        "#             # ! this is for UNet, not for Imagen\n",
        "#         ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "#         ModelKeys['cond_dim']=512\n",
        "#         # ModelKeys['embed_dim_position']=128\n",
        "#         # ModelKeys['text_embed_dim']=ModelKeys['cond_dim']-ModelKeys['embed_dim_position']\n",
        "#         # !!! Need to check this one = 1 or not\n",
        "#         ModelKeys['cond_images_channels']=0 # 1\n",
        "#         # !!! Need to check this one: should it be 8 or 64?\n",
        "#         ModelKeys['max_text_len']= DataKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # 8 # this is about text condi\n",
        "\n",
        "#         # for Imagen\n",
        "#         ModelKeys['pred_dim']=1 # for sequence, =1\n",
        "#         ModelKeys['diff_timesteps']=(96) # (96,)\n",
        "#         ModelKeys['loss_type']=0 # MSE\n",
        "#         ModelKeys['elucidated']=True #\n",
        "#         ModelKeys['padding_idx']=0\n",
        "#         ModelKeys['max_length']=DataKeys['max_AA_seq_len']\n",
        "#         ModelKeys['device']=device\n",
        "#         #\n",
        "#         # baskets for Model building\n",
        "#         ModelKeys['UNet']={}\n",
        "#         ModelKeys['Imagen']={}\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # prepare the Unet Key\n",
        "#         write_PK_UNet=dict()\n",
        "#         # used ones\n",
        "#         write_PK_UNet['dim']=ModelKeys['dim']\n",
        "#         write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim']\n",
        "#         write_PK_UNet['num_resnet_blocks']=1\n",
        "#         write_PK_UNet['cond_dim']=ModelKeys['cond_dim'] #this is where text embeddings are projected to...\n",
        "#          # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "#         # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "#         # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "#         # write_PK_UNet['out_dim']=None\n",
        "#         write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "#         write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_UNet['channels']=ModelKeys['pred_dim']\n",
        "#         write_PK_UNet['channels_out']=ModelKeys['pred_dim']\n",
        "\n",
        "#         write_PK_UNet['attn_dim_head']=64\n",
        "#         write_PK_UNet['attn_heads']=8\n",
        "#         write_PK_UNet['ff_mult']=2.\n",
        "#         write_PK_UNet['lowres_cond']=False # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "\n",
        "#         write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['layer_attns_depth']=1\n",
        "#         write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "#         write_PK_UNet['attend_at_middle']=True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "#         write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['use_linear_attn']=False\n",
        "#         write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "#         write_PK_UNet['cond_on_text'] = True\n",
        "#             # !!! Need to check this: 63, Imagen used a different one\n",
        "#         write_PK_UNet['max_text_len'] = DataKeys['max_AA_seq_len'] # ModelKeys['max_text_len']\n",
        "#         # write_PK_UNet['init_dim'] = None\n",
        "#         write_PK_UNet['resnet_groups'] = 8\n",
        "#         write_PK_UNet['init_conv_kernel_size'] =7     # kernel size of initial conv, if not using cross embed\n",
        "#         write_PK_UNet['init_cross_embed'] = False  #TODO - fix ouput size calcs for conv1d\n",
        "#         write_PK_UNet['init_cross_embed_kernel_sizes'] = (3, 7, 15)\n",
        "#         write_PK_UNet['cross_embed_downsample'] = False\n",
        "#         write_PK_UNet['cross_embed_downsample_kernel_sizes'] = (2, 4)\n",
        "\n",
        "#         write_PK_UNet['attn_pool_text'] = True\n",
        "#         write_PK_UNet['attn_pool_num_latents'] = 32 #32,   #perceiver model latents\n",
        "#         write_PK_UNet['dropout'] = 0.\n",
        "#         write_PK_UNet['memory_efficient'] = False\n",
        "#         write_PK_UNet['init_conv_to_final_conv_residual'] = False\n",
        "\n",
        "#         write_PK_UNet['use_global_context_attn'] = True\n",
        "#         write_PK_UNet['scale_skip_connection'] = True\n",
        "#         write_PK_UNet['final_resnet_block'] = True\n",
        "#         write_PK_UNet['final_conv_kernel_size'] = 3\n",
        "\n",
        "#         write_PK_UNet['cosine_sim_attn'] = True\n",
        "#         write_PK_UNet['self_cond'] = False\n",
        "#         write_PK_UNet['combine_upsample_fmaps'] = True      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "#         write_PK_UNet['pixel_shuffle_upsample'] = False     # may address checkboard artifacts\n",
        "#         # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "#         #\n",
        "#         ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # beyond UNet, for Whole model, all keys\n",
        "#         # \"None\" means defult value on function definition\n",
        "#         write_PK_Imagen=dict()\n",
        "\n",
        "#         # used ones\n",
        "#         write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "#         write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "#         write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "#         write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "#         write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "#         write_PK_Imagen['cond_dim']=ModelKeys['cond_dim'] # use default\n",
        "#             # can use default, 512; or like the below (check the code)\n",
        "#         write_PK_Imagen['text_embed_dim']=ModelKeys['text_embed_dim']-ModelKeys['embed_dim_position']\n",
        "#         # write_PK_Imagen['input_tokens']=None\n",
        "#         # write_PK_Imagen['sequence_embed']=None\n",
        "#         write_PK_Imagen['embed_dim_position']=ModelKeys['embed_dim_position']\n",
        "#         write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "#         write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "#         write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "#         # extend it to a full key\n",
        "#         ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "\n",
        "    elif CKeys['Problem_ID']==4:\n",
        "        pass\n",
        "\n",
        "#         ModelKeys={}\n",
        "#         # storage\n",
        "#         ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "#         # create the folder\n",
        "#         print(\"Creating the model dir...\")\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "#         # secondary folders\n",
        "#         ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "#         ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "#         # for UNet\n",
        "#         ModelKeys['dim']=768    # dim for UNet\n",
        "#         # for debug\n",
        "#         ModelKeys['dim']=256    # dim for UNet: GPU stat: 256:16093MiB;\n",
        "#         #\n",
        "#         ModelKeys['text_embed_dim']=512-128\n",
        "#         ModelKeys['embed_dim_position']=128\n",
        "#             # ! this is for UNet, not for Imagen\n",
        "#         ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "#         ModelKeys['cond_dim']=512\n",
        "#         # ModelKeys['embed_dim_position']=128\n",
        "#         # ModelKeys['text_embed_dim']=ModelKeys['cond_dim']-ModelKeys['embed_dim_position']\n",
        "#         # !!! Need to check this one = 1 or not\n",
        "#         ModelKeys['cond_images_channels']=0 # 1\n",
        "#         # !!! Need to check this one: should it be 8 or 64?\n",
        "#         ModelKeys['max_text_len']= DataKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # 8 # this is about text condi\n",
        "\n",
        "#         # for Imagen\n",
        "#         ModelKeys['pred_dim']=1 # for sequence, =1\n",
        "#         ModelKeys['diff_timesteps']=(96) # (96,)\n",
        "#         ModelKeys['loss_type']=0 # MSE\n",
        "#         ModelKeys['elucidated']=True #\n",
        "#         ModelKeys['padding_idx']=0\n",
        "#         ModelKeys['max_length']=DataKeys['max_AA_seq_len']\n",
        "#         ModelKeys['device']=device\n",
        "#         #\n",
        "#         # baskets for Model building\n",
        "#         ModelKeys['UNet']={}\n",
        "#         ModelKeys['Imagen']={}\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # prepare the Unet Key\n",
        "#         write_PK_UNet=dict()\n",
        "#         # used ones\n",
        "#         write_PK_UNet['dim']=ModelKeys['dim']\n",
        "#         write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim']\n",
        "#         write_PK_UNet['num_resnet_blocks']=1\n",
        "#         write_PK_UNet['cond_dim']=ModelKeys['cond_dim'] #this is where text embeddings are projected to...\n",
        "#          # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "#         # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "#         # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "#         # write_PK_UNet['out_dim']=None\n",
        "#         write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "#         write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_UNet['channels']=ModelKeys['pred_dim']\n",
        "#         write_PK_UNet['channels_out']=ModelKeys['pred_dim']\n",
        "\n",
        "#         write_PK_UNet['attn_dim_head']=64\n",
        "#         write_PK_UNet['attn_heads']=8\n",
        "#         write_PK_UNet['ff_mult']=2.\n",
        "#         write_PK_UNet['lowres_cond']=False # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "\n",
        "#         write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['layer_attns_depth']=1\n",
        "#         write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "#         write_PK_UNet['attend_at_middle']=True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "#         write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['use_linear_attn']=False\n",
        "#         write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "#         write_PK_UNet['cond_on_text'] = True\n",
        "#             # !!! Need to check this: 63, Imagen used a different one\n",
        "#         write_PK_UNet['max_text_len'] = DataKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # ModelKeys['max_text_len']\n",
        "#         # write_PK_UNet['init_dim'] = None\n",
        "#         write_PK_UNet['resnet_groups'] = 8\n",
        "#         write_PK_UNet['init_conv_kernel_size'] =7     # kernel size of initial conv, if not using cross embed\n",
        "#         write_PK_UNet['init_cross_embed'] = False  #TODO - fix ouput size calcs for conv1d\n",
        "#         write_PK_UNet['init_cross_embed_kernel_sizes'] = (3, 7, 15)\n",
        "#         write_PK_UNet['cross_embed_downsample'] = False\n",
        "#         write_PK_UNet['cross_embed_downsample_kernel_sizes'] = (2, 4)\n",
        "\n",
        "#         write_PK_UNet['attn_pool_text'] = True\n",
        "#         write_PK_UNet['attn_pool_num_latents'] = 32 #32,   #perceiver model latents\n",
        "#         write_PK_UNet['dropout'] = 0.\n",
        "#         write_PK_UNet['memory_efficient'] = False\n",
        "#         write_PK_UNet['init_conv_to_final_conv_residual'] = False\n",
        "\n",
        "#         write_PK_UNet['use_global_context_attn'] = True\n",
        "#         write_PK_UNet['scale_skip_connection'] = True\n",
        "#         write_PK_UNet['final_resnet_block'] = True\n",
        "#         write_PK_UNet['final_conv_kernel_size'] = 3\n",
        "\n",
        "#         write_PK_UNet['cosine_sim_attn'] = True\n",
        "#         write_PK_UNet['self_cond'] = False\n",
        "#         write_PK_UNet['combine_upsample_fmaps'] = True      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "#         write_PK_UNet['pixel_shuffle_upsample'] = False     # may address checkboard artifacts\n",
        "#         # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "#         #\n",
        "#         ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # beyond UNet, for Whole model, all keys\n",
        "#         # \"None\" means defult value on function definition\n",
        "#         write_PK_Imagen=dict()\n",
        "\n",
        "#         # used ones\n",
        "#         write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "#         write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "#         write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "#         write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "#         write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "#         write_PK_Imagen['cond_dim']=ModelKeys['cond_dim'] # use default\n",
        "#             # can use default, 512; or like the below (check the code)\n",
        "#         write_PK_Imagen['text_embed_dim']=ModelKeys['text_embed_dim']-ModelKeys['embed_dim_position']\n",
        "#         # write_PK_Imagen['input_tokens']=None\n",
        "#         # write_PK_Imagen['sequence_embed']=None\n",
        "#         write_PK_Imagen['embed_dim_position']=ModelKeys['embed_dim_position']\n",
        "#         write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "#         write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "#         write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "#         # extend it to a full key\n",
        "#         ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "\n",
        "    elif CKeys['Problem_ID']==5:\n",
        "        pass\n",
        "\n",
        "#         # this is 1st run....\n",
        "#         # +++++++++++++++++++++++++++++++++++++\n",
        "#         # SecStr as input seq\n",
        "#         ModelKeys={}\n",
        "#         # storage\n",
        "#         ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "#         # create the folder\n",
        "#         print(\"Creating the model dir...\")\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "#         # secondary folders\n",
        "#         ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "#         ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "#         # for UNet\n",
        "#         ModelKeys['dim']=256    # dim for UNet\n",
        "#         ModelKeys['text_embed_dim'] = 512\n",
        "#         ModelKeys['embed_dim_position']=32\n",
        "#         ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "#         ModelKeys['cond_dim'] = 512\n",
        "#         ModelKeys['cond_images_channels']=DataKeys['image_channels'] # 1280 # for embedding dim # 1\n",
        "#         ModelKeys['max_text_len']=DataKeys['max_AA_seq_len'] # this is about text condi\n",
        "#         # + for embedding\n",
        "#         ModelKeys['image_channels']=DataKeys['image_channels'] # 1280\n",
        "#         # for Imagen\n",
        "#         ModelKeys['pred_dim']= ModelKeys['image_channels'] # 1 # for sequence, =1\n",
        "#         ModelKeys['diff_timesteps']=(96,)\n",
        "#         ModelKeys['loss_type']=0 # MSE\n",
        "#         ModelKeys['elucidated']=True #\n",
        "#         ModelKeys['padding_idx']=0\n",
        "#         ModelKeys['max_length']=DataKeys['max_AA_seq_len'] # max seq len\n",
        "#         ModelKeys['device']=device\n",
        "#         #\n",
        "#         # buckets for Model building\n",
        "#         ModelKeys['UNet']={}\n",
        "#         ModelKeys['Imagen']={}\n",
        "#         # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#         write_PK_UNet=dict()\n",
        "#         write_PK_UNet['dim']=ModelKeys['dim'] # 256\n",
        "#         write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim'] # 512+32\n",
        "#         write_PK_UNet['num_resnet_blocks']=1\n",
        "#         write_PK_UNet['cond_dim']=ModelKeys['cond_dim']  #this is where text embeddings are projected to...\n",
        "#         # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "#         # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "#         # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "#         # write_PK_UNet['out_dim']=None\n",
        "#         write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "#         write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_UNet['channels']=ModelKeys['image_channels'] # 1\n",
        "#         write_PK_UNet['channels_out']=ModelKeys['image_channels'] # 1\n",
        "\n",
        "#         write_PK_UNet['attn_dim_head']=64\n",
        "#         write_PK_UNet['attn_heads']=8\n",
        "#         write_PK_UNet['ff_mult']=2.\n",
        "#         write_PK_UNet['lowres_cond']=False  # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "#         write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['layer_attns_depth']=1\n",
        "#         write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "#         write_PK_UNet['attend_at_middle']=True          # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "#         write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['use_linear_attn']=False\n",
        "#         write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "#         write_PK_UNet['cond_on_text']=True\n",
        "#         write_PK_UNet['max_text_len']=ModelKeys['max_text_len'] # need to check this one\n",
        "#         # ModelKeys['UNet']['init_dim']=None\n",
        "#         write_PK_UNet['resnet_groups']=8\n",
        "#         write_PK_UNet['init_conv_kernel_size']=7 # kernel size of initial conv, if not using cross embed\n",
        "#         write_PK_UNet['init_cross_embed']=False  #TODO - fix ouput size calcs for conv1d\n",
        "#         write_PK_UNet['init_cross_embed_kernel_sizes']=(3, 7, 15)\n",
        "#         write_PK_UNet['cross_embed_downsample']=False\n",
        "#         write_PK_UNet['cross_embed_downsample_kernel_sizes']=(2, 4)\n",
        "\n",
        "#         write_PK_UNet['attn_pool_text']=True\n",
        "#         write_PK_UNet['attn_pool_num_latents']=32 #perceiver model latents\n",
        "#         write_PK_UNet['dropout']=0.\n",
        "#         write_PK_UNet['memory_efficient']=False\n",
        "#         write_PK_UNet['init_conv_to_final_conv_residual']=False\n",
        "\n",
        "#         write_PK_UNet['use_global_context_attn']=True\n",
        "#         write_PK_UNet['scale_skip_connection']=True\n",
        "#         write_PK_UNet['final_resnet_block']=True\n",
        "#         write_PK_UNet['final_conv_kernel_size']=3\n",
        "\n",
        "#         write_PK_UNet['cosine_sim_attn']=True\n",
        "#         write_PK_UNet['self_cond']=False\n",
        "#         write_PK_UNet['combine_upsample_fmaps']=True    # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "#         write_PK_UNet['pixel_shuffle_upsample']=False   # may address checkboard artifacts\n",
        "#         # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "#         ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "#         # +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "#         # beyond UNet, for Whole model, all keys\n",
        "#         # \"None\" means defult value on function definition\n",
        "#         write_PK_Imagen=dict()\n",
        "\n",
        "#         write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "#         write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "#         write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "#         write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "#         write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "#         # write_PK_Imagen['cond_dim']=None # use default\n",
        "#         # write_PK_Imagen['text_embed_dim']=None\n",
        "#         # write_PK_Imagen['input_tokens']=None\n",
        "#         # write_PK_Imagen['sequence_embed']=None\n",
        "#         # write_PK_Imagen['embed_dim_position']=None\n",
        "#         write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "#         write_PK_Imagen['cond_images_channels']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "#         write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "#         # extend it to a full key\n",
        "#         ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "\n",
        "    elif CKeys['Problem_ID']==6:\n",
        "        # =====================================================\n",
        "        # ForcePath --> AA sequence\n",
        "        # =====================================================\n",
        "        #\n",
        "        # this is 1st run....\n",
        "        # +++++++++++++++++++++++++++++++++++++\n",
        "        # ForcPath as input seq\n",
        "        ModelKeys={}\n",
        "        # storage\n",
        "        ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "        # create the folder\n",
        "        print(\"Creating the model dir...\")\n",
        "        UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "        # secondary folders\n",
        "        ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "        UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "        ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "        UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "        # for UNet\n",
        "        ModelKeys['dim']=256    # dim for UNet\n",
        "        ModelKeys['text_embed_dim'] = 512\n",
        "        ModelKeys['embed_dim_position']=32\n",
        "        ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "        ModelKeys['cond_dim'] = 512\n",
        "        ModelKeys['cond_images_channels']=DataKeys['image_channels'] # 1\n",
        "        ModelKeys['max_text_len']=DataKeys['max_AA_seq_len'] # this is about text condi\n",
        "        # for Imagen\n",
        "        ModelKeys['pred_dim']=DataKeys['image_channels'] # 1 # for sequence, =1\n",
        "        ModelKeys['diff_timesteps']=(96,)\n",
        "        ModelKeys['loss_type']=0 # MSE\n",
        "        ModelKeys['elucidated']=True #\n",
        "        ModelKeys['padding_idx']=0\n",
        "        ModelKeys['max_length']=DataKeys['max_AA_seq_len'] # max seq len\n",
        "        ModelKeys['device']=device\n",
        "        #\n",
        "        # buckets for Model building\n",
        "        ModelKeys['UNet']={}\n",
        "        ModelKeys['Imagen']={}\n",
        "        # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        write_PK_UNet=dict()\n",
        "        write_PK_UNet['dim']=ModelKeys['dim'] # 256\n",
        "        write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim'] # 512+32\n",
        "        write_PK_UNet['num_resnet_blocks']=1\n",
        "        write_PK_UNet['cond_dim']=ModelKeys['cond_dim']  #this is where text embeddings are projected to...\n",
        "        # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "        # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "        # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "        # write_PK_UNet['out_dim']=None\n",
        "        write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "        write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "        write_PK_UNet['channels']=DataKeys['image_channels'] # 1\n",
        "        write_PK_UNet['channels_out']=DataKeys['image_channels'] # 1\n",
        "\n",
        "        write_PK_UNet['attn_dim_head']=64\n",
        "        write_PK_UNet['attn_heads']=8\n",
        "        write_PK_UNet['ff_mult']=2.\n",
        "        write_PK_UNet['lowres_cond']=False  # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "        write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "        write_PK_UNet['layer_attns_depth']=1\n",
        "        write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "        write_PK_UNet['attend_at_middle']=True          # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "        write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "        write_PK_UNet['use_linear_attn']=False\n",
        "        write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "        write_PK_UNet['cond_on_text']=True\n",
        "        write_PK_UNet['max_text_len']=ModelKeys['max_text_len'] # need to check this one\n",
        "        # ModelKeys['UNet']['init_dim']=None\n",
        "        write_PK_UNet['resnet_groups']=8\n",
        "        write_PK_UNet['init_conv_kernel_size']=7 # kernel size of initial conv, if not using cross embed\n",
        "        write_PK_UNet['init_cross_embed']=False  #TODO - fix ouput size calcs for conv1d\n",
        "        write_PK_UNet['init_cross_embed_kernel_sizes']=(3, 7, 15)\n",
        "        write_PK_UNet['cross_embed_downsample']=False\n",
        "        write_PK_UNet['cross_embed_downsample_kernel_sizes']=(2, 4)\n",
        "\n",
        "        write_PK_UNet['attn_pool_text']=True\n",
        "        write_PK_UNet['attn_pool_num_latents']=32 #perceiver model latents\n",
        "        write_PK_UNet['dropout']=0.\n",
        "        write_PK_UNet['memory_efficient']=False\n",
        "        write_PK_UNet['init_conv_to_final_conv_residual']=False\n",
        "\n",
        "        write_PK_UNet['use_global_context_attn']=True\n",
        "        write_PK_UNet['scale_skip_connection']=True\n",
        "        write_PK_UNet['final_resnet_block']=True\n",
        "        write_PK_UNet['final_conv_kernel_size']=3\n",
        "\n",
        "        write_PK_UNet['cosine_sim_attn']=True\n",
        "        write_PK_UNet['self_cond']=False\n",
        "        write_PK_UNet['combine_upsample_fmaps']=True    # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "        write_PK_UNet['pixel_shuffle_upsample']=False   # may address checkboard artifacts\n",
        "        # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "        ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        # beyond UNet, for Whole model, all keys\n",
        "        # \"None\" means defult value on function definition\n",
        "        write_PK_Imagen=dict()\n",
        "\n",
        "        write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "        write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "        write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "        write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "        write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "        write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "        # write_PK_Imagen['cond_dim']=None # use default\n",
        "        # write_PK_Imagen['text_embed_dim']=None\n",
        "        # write_PK_Imagen['input_tokens']=None\n",
        "        # write_PK_Imagen['sequence_embed']=None\n",
        "        # write_PK_Imagen['embed_dim_position']=None\n",
        "        write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "        write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "        write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "        write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "        # extend it to a full key\n",
        "        ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "        # to be defined\n",
        "\n",
        "    elif CKeys['Problem_ID']==7:\n",
        "        pass\n",
        "#         # ================================================================\n",
        "#         # SecStr text summary --> AA seq pLM embedding\n",
        "#         # ================================================================\n",
        "#         ModelKeys={}\n",
        "#         # storage\n",
        "#         ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "#         # create the folder\n",
        "#         print(\"Creating the model dir...\")\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "#         # secondary folders\n",
        "#         ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "#         ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "#         UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "#         # for UNet\n",
        "#         ModelKeys['dim']=768    # dim for UNet\n",
        "#         # for debug\n",
        "#         ModelKeys['dim']=256    # dim for UNet: GPU stat: 256:16093MiB;\n",
        "#         #\n",
        "#         ModelKeys['text_embed_dim']=512-128\n",
        "#         ModelKeys['embed_dim_position']=128\n",
        "#             # ! this is for UNet, not for Imagen\n",
        "#         ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "#         ModelKeys['cond_dim']=512\n",
        "#         # ModelKeys['embed_dim_position']=128\n",
        "#         # ModelKeys['text_embed_dim']=ModelKeys['cond_dim']-ModelKeys['embed_dim_position']\n",
        "#         # !!! Need to check this one = 1 or not\n",
        "#         # $: UNet use this key to decide whether cond_img is provided or not (0)\n",
        "#         ModelKeys['cond_images_channels']= 0 # indicate NO cond_img  # 0 # DataKeys['image_channels']\n",
        "#         # !!! Need to check this one: should it be 8 or 64?\n",
        "#         ModelKeys['max_text_len']= DataKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # 8 # this is about text condi\n",
        "\n",
        "#         # for Imagen\n",
        "#         ModelKeys['pred_dim']=DataKeys['image_channels'] # 1 # for sequence, =1\n",
        "#         ModelKeys['diff_timesteps']=(96) # (96,)\n",
        "#         ModelKeys['loss_type']=0 # MSE\n",
        "#         ModelKeys['elucidated']=True #\n",
        "#         ModelKeys['padding_idx']=0\n",
        "#         ModelKeys['max_length']=DataKeys['max_AA_seq_len']\n",
        "#         ModelKeys['device']=device\n",
        "#         #\n",
        "#         # baskets for Model building\n",
        "#         ModelKeys['UNet']={}\n",
        "#         ModelKeys['Imagen']={}\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # prepare the Unet Key\n",
        "#         write_PK_UNet=dict()\n",
        "#         # used ones\n",
        "#         write_PK_UNet['dim']=ModelKeys['dim']\n",
        "#         write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim']\n",
        "#         write_PK_UNet['num_resnet_blocks']=1\n",
        "#         write_PK_UNet['cond_dim']=ModelKeys['cond_dim'] #this is where text embeddings are projected to...\n",
        "#          # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "#         # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "#         # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "#         # write_PK_UNet['out_dim']=None\n",
        "#         write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "#         write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_UNet['channels']=DataKeys['image_channels'] # ModelKeys['pred_dim']\n",
        "#         write_PK_UNet['channels_out']=DataKeys['image_channels'] # ModelKeys['pred_dim']\n",
        "\n",
        "#         write_PK_UNet['attn_dim_head']=64\n",
        "#         write_PK_UNet['attn_heads']=8\n",
        "#         write_PK_UNet['ff_mult']=2.\n",
        "#         write_PK_UNet['lowres_cond']=False # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "\n",
        "#         write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['layer_attns_depth']=1\n",
        "#         write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "#         write_PK_UNet['attend_at_middle']=True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "#         write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "#         write_PK_UNet['use_linear_attn']=False\n",
        "#         write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "#         write_PK_UNet['cond_on_text'] = True\n",
        "#             # !!! Need to check this: 63, Imagen used a different one\n",
        "#             # !!! here try the new one which seems to be correct\n",
        "#         write_PK_UNet['max_text_len'] = ModelKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # ModelKeys['max_text_len']\n",
        "#         # write_PK_UNet['init_dim'] = None\n",
        "#         write_PK_UNet['resnet_groups'] = 8\n",
        "#         write_PK_UNet['init_conv_kernel_size'] =7     # kernel size of initial conv, if not using cross embed\n",
        "#         write_PK_UNet['init_cross_embed'] = False  #TODO - fix ouput size calcs for conv1d\n",
        "#         write_PK_UNet['init_cross_embed_kernel_sizes'] = (3, 7, 15)\n",
        "#         write_PK_UNet['cross_embed_downsample'] = False\n",
        "#         write_PK_UNet['cross_embed_downsample_kernel_sizes'] = (2, 4)\n",
        "\n",
        "#         write_PK_UNet['attn_pool_text'] = True\n",
        "#         write_PK_UNet['attn_pool_num_latents'] = 32 #32,   #perceiver model latents\n",
        "#         write_PK_UNet['dropout'] = 0.\n",
        "#         write_PK_UNet['memory_efficient'] = False\n",
        "#         write_PK_UNet['init_conv_to_final_conv_residual'] = False\n",
        "\n",
        "#         write_PK_UNet['use_global_context_attn'] = True\n",
        "#         write_PK_UNet['scale_skip_connection'] = True\n",
        "#         write_PK_UNet['final_resnet_block'] = True\n",
        "#         write_PK_UNet['final_conv_kernel_size'] = 3\n",
        "\n",
        "#         write_PK_UNet['cosine_sim_attn'] = True\n",
        "#         write_PK_UNet['self_cond'] = False\n",
        "#         write_PK_UNet['combine_upsample_fmaps'] = True      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "#         write_PK_UNet['pixel_shuffle_upsample'] = False     # may address checkboard artifacts\n",
        "#         # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "#         #\n",
        "#         ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "#         # ++++++++++++++++++++++++++++++++++++++\n",
        "#         # beyond UNet, for Whole model, all keys\n",
        "#         # \"None\" means defult value on function definition\n",
        "#         write_PK_Imagen=dict()\n",
        "\n",
        "#         # used ones\n",
        "#         write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "#         write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "#         write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "#         write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "#         write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "#         write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "#         write_PK_Imagen['cond_dim']=ModelKeys['cond_dim'] # use default\n",
        "#             # can use default, 512; or like the below (check the code)\n",
        "#         write_PK_Imagen['text_embed_dim']=ModelKeys['text_embed_dim']-ModelKeys['embed_dim_position']\n",
        "#         # write_PK_Imagen['input_tokens']=None\n",
        "#         # write_PK_Imagen['sequence_embed']=None\n",
        "#         write_PK_Imagen['embed_dim_position']=ModelKeys['embed_dim_position']\n",
        "#         write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "#         write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "#         write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "#         write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "#         # extend it to a full key\n",
        "#         ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "\n",
        "    elif CKeys['Problem_ID']==8:\n",
        "        # ================================================================\n",
        "        # FmaxEne text summary --> AA seq pLM embedding\n",
        "        # ================================================================\n",
        "        ModelKeys={}\n",
        "        # storage\n",
        "        ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "        # create the folder\n",
        "        print(\"Creating the model dir...\")\n",
        "        UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "        # secondary folders\n",
        "        ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "        UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "        ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "        UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "        # for UNet\n",
        "        ModelKeys['dim']=768    # dim for UNet\n",
        "        # for debug\n",
        "        ModelKeys['dim']=256    # dim for UNet: GPU stat: 256:16093MiB;\n",
        "        #\n",
        "        ModelKeys['text_embed_dim']=512-128\n",
        "        ModelKeys['embed_dim_position']=128\n",
        "            # ! this is for UNet, not for Imagen\n",
        "        ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "        ModelKeys['cond_dim']=512\n",
        "        # ModelKeys['embed_dim_position']=128\n",
        "        # ModelKeys['text_embed_dim']=ModelKeys['cond_dim']-ModelKeys['embed_dim_position']\n",
        "        # !!! Need to check this one = 1 or not\n",
        "        # $: UNet use this key to decide whether cond_img is provided or not (0)\n",
        "        ModelKeys['cond_images_channels']=0 # indicate no cond_img is used # 1\n",
        "        # !!! Need to check this one: should it be 8 or 64?\n",
        "        ModelKeys['max_text_len']= DataKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # 8 # this is about text condi\n",
        "\n",
        "        # for Imagen\n",
        "        ModelKeys['pred_dim']=DataKeys['image_channels'] # 1 # for sequence, =1\n",
        "        ModelKeys['diff_timesteps']=(96) # (96,)\n",
        "        ModelKeys['loss_type']=0 # MSE\n",
        "        ModelKeys['elucidated']=True #\n",
        "        ModelKeys['padding_idx']=0\n",
        "        ModelKeys['max_length']=DataKeys['max_AA_seq_len']\n",
        "        ModelKeys['device']=device\n",
        "        #\n",
        "        # baskets for Model building\n",
        "        ModelKeys['UNet']={}\n",
        "        ModelKeys['Imagen']={}\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # prepare the Unet Key\n",
        "        write_PK_UNet=dict()\n",
        "        # used ones\n",
        "        write_PK_UNet['dim']=ModelKeys['dim']\n",
        "        write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim']\n",
        "        write_PK_UNet['num_resnet_blocks']=1\n",
        "        write_PK_UNet['cond_dim']=ModelKeys['cond_dim'] #this is where text embeddings are projected to...\n",
        "         # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "        # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "        # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "        # write_PK_UNet['out_dim']=None\n",
        "        write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "        write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "        write_PK_UNet['channels']=DataKeys['image_channels']  # ModelKeys['pred_dim']\n",
        "        write_PK_UNet['channels_out']=DataKeys['image_channels'] # ModelKeys['pred_dim']\n",
        "\n",
        "        write_PK_UNet['attn_dim_head']=64\n",
        "        write_PK_UNet['attn_heads']=8\n",
        "        write_PK_UNet['ff_mult']=2.\n",
        "        write_PK_UNet['lowres_cond']=False # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "\n",
        "        write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "        write_PK_UNet['layer_attns_depth']=1\n",
        "        write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "        write_PK_UNet['attend_at_middle']=True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "        write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "        write_PK_UNet['use_linear_attn']=False\n",
        "        write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "        write_PK_UNet['cond_on_text'] = True\n",
        "            # !!! Need to check this: 63, Imagen used a different one\n",
        "        write_PK_UNet['max_text_len'] = DataKeys['max_text_len'] # DataKeys['max_AA_seq_len'] # ModelKeys['max_text_len']\n",
        "        # write_PK_UNet['init_dim'] = None\n",
        "        write_PK_UNet['resnet_groups'] = 8\n",
        "        write_PK_UNet['init_conv_kernel_size'] =7     # kernel size of initial conv, if not using cross embed\n",
        "        write_PK_UNet['init_cross_embed'] = False  #TODO - fix ouput size calcs for conv1d\n",
        "        write_PK_UNet['init_cross_embed_kernel_sizes'] = (3, 7, 15)\n",
        "        write_PK_UNet['cross_embed_downsample'] = False\n",
        "        write_PK_UNet['cross_embed_downsample_kernel_sizes'] = (2, 4)\n",
        "\n",
        "        write_PK_UNet['attn_pool_text'] = True\n",
        "        write_PK_UNet['attn_pool_num_latents'] = 32 #32,   #perceiver model latents\n",
        "        write_PK_UNet['dropout'] = 0.\n",
        "        write_PK_UNet['memory_efficient'] = False\n",
        "        write_PK_UNet['init_conv_to_final_conv_residual'] = False\n",
        "\n",
        "        write_PK_UNet['use_global_context_attn'] = True\n",
        "        write_PK_UNet['scale_skip_connection'] = True\n",
        "        write_PK_UNet['final_resnet_block'] = True\n",
        "        write_PK_UNet['final_conv_kernel_size'] = 3\n",
        "\n",
        "        write_PK_UNet['cosine_sim_attn'] = True\n",
        "        write_PK_UNet['self_cond'] = False\n",
        "        write_PK_UNet['combine_upsample_fmaps'] = True      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "        write_PK_UNet['pixel_shuffle_upsample'] = False     # may address checkboard artifacts\n",
        "        # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "        #\n",
        "        ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "        # ++++++++++++++++++++++++++++++++++++++\n",
        "        # beyond UNet, for Whole model, all keys\n",
        "        # \"None\" means defult value on function definition\n",
        "        write_PK_Imagen=dict()\n",
        "\n",
        "        # used ones\n",
        "        write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "        write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "        write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "        write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "        write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "        write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "        write_PK_Imagen['cond_dim']=ModelKeys['cond_dim'] # use default\n",
        "            # can use default, 512; or like the below (check the code)\n",
        "        write_PK_Imagen['text_embed_dim']=ModelKeys['text_embed_dim']-ModelKeys['embed_dim_position']\n",
        "        # write_PK_Imagen['input_tokens']=None\n",
        "        # write_PK_Imagen['sequence_embed']=None\n",
        "        write_PK_Imagen['embed_dim_position']=ModelKeys['embed_dim_position']\n",
        "        write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "        write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "        write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "        write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "        # extend it to a full key\n",
        "        ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "\n",
        "    elif CKeys['Problem_ID']==11:\n",
        "        # =====================================================\n",
        "        # ForcePath --> AA sequence\n",
        "        # =====================================================\n",
        "        #\n",
        "        # this is 1st run....\n",
        "        # +++++++++++++++++++++++++++++++++++++\n",
        "        # ForcPath as input seq\n",
        "        ModelKeys={}\n",
        "        # storage\n",
        "        ModelKeys['model_dir']=PKeys['prefix']+'1_model_SS/'\n",
        "        # create the folder\n",
        "        print(\"Creating the model dir...\")\n",
        "        UtilityPack.create_path(ModelKeys['model_dir'])\n",
        "        # secondary folders\n",
        "        ModelKeys['model_dir_sample']=ModelKeys['model_dir']+'0_mid_sample/'\n",
        "        UtilityPack.create_path(ModelKeys['model_dir_sample'])\n",
        "        ModelKeys['model_dir_model']=ModelKeys['model_dir']+'1_store_model/'\n",
        "        UtilityPack.create_path(ModelKeys['model_dir_model'])\n",
        "\n",
        "        # for UNet\n",
        "        ModelKeys['dim']=256    # dim for UNet\n",
        "        ModelKeys['text_embed_dim'] = 512\n",
        "        ModelKeys['embed_dim_position']=32\n",
        "        ModelKeys['text_embed_dim']=ModelKeys['text_embed_dim']+ModelKeys['embed_dim_position']\n",
        "        ModelKeys['cond_dim'] = 512\n",
        "        ModelKeys['cond_images_channels']=DataKeys['image_channels'] # 1\n",
        "        ModelKeys['max_text_len']=DataKeys['max_AA_seq_len'] # this is about text condi\n",
        "        # for Imagen\n",
        "        ModelKeys['pred_dim']=DataKeys['image_channels'] # 1 # for sequence, =1\n",
        "        ModelKeys['diff_timesteps']=(96,)\n",
        "        ModelKeys['loss_type']=0 # MSE\n",
        "        ModelKeys['elucidated']=True #\n",
        "        ModelKeys['padding_idx']=0\n",
        "        ModelKeys['max_length']=DataKeys['max_AA_seq_len'] # max seq len\n",
        "        ModelKeys['device']=device\n",
        "        #\n",
        "        # buckets for Model building\n",
        "        ModelKeys['UNet']={}\n",
        "        ModelKeys['Imagen']={}\n",
        "        # ++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        write_PK_UNet=dict()\n",
        "        write_PK_UNet['dim']=ModelKeys['dim'] # 256\n",
        "        write_PK_UNet['text_embed_dim']=ModelKeys['text_embed_dim'] # 512+32\n",
        "        write_PK_UNet['num_resnet_blocks']=1\n",
        "        write_PK_UNet['cond_dim']=ModelKeys['cond_dim']  #this is where text embeddings are projected to...\n",
        "        # write_PK_UNet['num_image_tokens']=None # using the default value\n",
        "        # write_PK_UNet['num_time_tokens']=None  # using the default\n",
        "        # write_PK_UNet['learned_sinu_pos_emb_dim']=None\n",
        "        # write_PK_UNet['out_dim']=None\n",
        "        write_PK_UNet['dim_mults']=(1, 2, 4, 8)\n",
        "\n",
        "        write_PK_UNet['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "        write_PK_UNet['channels']=DataKeys['image_channels'] # 1\n",
        "        write_PK_UNet['channels_out']=DataKeys['image_channels'] # 1\n",
        "\n",
        "        write_PK_UNet['attn_dim_head']=64\n",
        "        write_PK_UNet['attn_heads']=8\n",
        "        write_PK_UNet['ff_mult']=2.\n",
        "        write_PK_UNet['lowres_cond']=False  # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
        "\n",
        "        write_PK_UNet['layer_attns']=(False, True, True, False)\n",
        "        write_PK_UNet['layer_attns_depth']=1\n",
        "        write_PK_UNet['layer_attns_add_text_cond']=True # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
        "        write_PK_UNet['attend_at_middle']=True          # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "        write_PK_UNet['layer_cross_attns']=(False, True, True, False)\n",
        "        write_PK_UNet['use_linear_attn']=False\n",
        "        write_PK_UNet['use_linear_cross_attn']=False\n",
        "\n",
        "        write_PK_UNet['cond_on_text']=True\n",
        "        write_PK_UNet['max_text_len']=ModelKeys['max_text_len'] # need to check this one\n",
        "        # ModelKeys['UNet']['init_dim']=None\n",
        "        write_PK_UNet['resnet_groups']=8\n",
        "        write_PK_UNet['init_conv_kernel_size']=7 # kernel size of initial conv, if not using cross embed\n",
        "        write_PK_UNet['init_cross_embed']=False  #TODO - fix ouput size calcs for conv1d\n",
        "        write_PK_UNet['init_cross_embed_kernel_sizes']=(3, 7, 15)\n",
        "        write_PK_UNet['cross_embed_downsample']=False\n",
        "        write_PK_UNet['cross_embed_downsample_kernel_sizes']=(2, 4)\n",
        "\n",
        "        write_PK_UNet['attn_pool_text']=True\n",
        "        write_PK_UNet['attn_pool_num_latents']=32 #perceiver model latents\n",
        "        write_PK_UNet['dropout']=0.\n",
        "        write_PK_UNet['memory_efficient']=False\n",
        "        write_PK_UNet['init_conv_to_final_conv_residual']=False\n",
        "\n",
        "        write_PK_UNet['use_global_context_attn']=True\n",
        "        write_PK_UNet['scale_skip_connection']=True\n",
        "        write_PK_UNet['final_resnet_block']=True\n",
        "        write_PK_UNet['final_conv_kernel_size']=3\n",
        "\n",
        "        write_PK_UNet['cosine_sim_attn']=True\n",
        "        write_PK_UNet['self_cond']=False\n",
        "        write_PK_UNet['combine_upsample_fmaps']=True    # combine feature maps from all upsample blocks, used in unet squared successfully\n",
        "        write_PK_UNet['pixel_shuffle_upsample']=False   # may address checkboard artifacts\n",
        "        # write_PK_UNet['beginning_and_final_conv_present']=None # use default\n",
        "        ModelKeys['UNet']=UtilityPack.prepare_UNet_keys(write_PK_UNet)\n",
        "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "        # beyond UNet, for Whole model, all keys\n",
        "        # \"None\" means defult value on function definition\n",
        "        write_PK_Imagen=dict()\n",
        "\n",
        "        write_PK_Imagen['timesteps']=ModelKeys['diff_timesteps']\n",
        "        write_PK_Imagen['dim']=ModelKeys['dim']\n",
        "        write_PK_Imagen['pred_dim']=ModelKeys['pred_dim']\n",
        "        write_PK_Imagen['loss_type']=ModelKeys['loss_type'] # 0 # MSE\n",
        "        write_PK_Imagen['elucidated']=ModelKeys['elucidated'] # True\n",
        "        write_PK_Imagen['padding_idx']=ModelKeys['padding_idx'] # 0 # need to check\n",
        "        # write_PK_Imagen['cond_dim']=None # use default\n",
        "        # write_PK_Imagen['text_embed_dim']=None\n",
        "        # write_PK_Imagen['input_tokens']=None\n",
        "        # write_PK_Imagen['sequence_embed']=None\n",
        "        # write_PK_Imagen['embed_dim_position']=None\n",
        "        write_PK_Imagen['max_text_len']=ModelKeys['max_text_len']\n",
        "        write_PK_Imagen['cond_images_channels']=ModelKeys['cond_images_channels']\n",
        "        write_PK_Imagen['max_length']=ModelKeys['max_length']\n",
        "        write_PK_Imagen['device']=ModelKeys['device']\n",
        "\n",
        "        # extend it to a full key\n",
        "        ModelKeys['Imagen']=UtilityPack.prepare_ModelB_keys(write_PK_Imagen)\n",
        "        # to be defined\n",
        "\n",
        "\n",
        "    # --\n",
        "    print(\"==================================================\")\n",
        "    print(\"store the MODEL key for the next-time usage\")\n",
        "    print(\"==================================================\")\n",
        "    model_pack = {}\n",
        "    model_pack['ModelKeys']=ModelKeys\n",
        "    with open(PKeys['pk_model_pack'], 'wb') as handle:\n",
        "        pickle.dump(model_pack, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "\n",
        "else: # both for training and testing\n",
        "    #\n",
        "    print(\"==================================================\")\n",
        "    print(\"load in the MODEL key from the previous storage\")\n",
        "    print(\"==================================================\")\n",
        "    #\n",
        "    # this is not the first run\n",
        "    print('This is not the first run')\n",
        "    print('Load back in the model packages...')\n",
        "    with open(PKeys['pk_model_pack'], 'rb') as handle:\n",
        "        model_pack = pickle.load(handle)\n",
        "    # deliver the results\n",
        "    ModelKeys=model_pack['ModelKeys']\n",
        "    print('Done.')\n",
        "\n"
      ],
      "metadata": {
        "id": "5xnnV0nBjWff",
        "outputId": "1e315b5a-7f9a-4990-b0fc-01777e49fb58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5xnnV0nBjWff",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "load in the MODEL key from the previous storage\n",
            "==================================================\n",
            "This is not the first run\n",
            "Load back in the model packages...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(DataKeys)\n",
        "# check a few key words\n",
        "# print (DataKeys['max_text_len'])\n",
        "# print(DataKeys['max_text_len'])\n",
        "print(ModelKeys['max_text_len'])\n",
        "print(DataKeys['image_channels'])\n",
        "print(ModelKeys['cond_images_channels'])\n",
        "print(ModelKeys['pred_dim'])"
      ],
      "metadata": {
        "id": "G1tGRPWCjWiW",
        "outputId": "5e85cd7d-6b81-432d-ed0f-53a147a962ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G1tGRPWCjWiW",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "33\n",
            "33\n",
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(ModelKeys['UNet'], indent=4))\n",
        "# print(json.dumps(ModelKeys['Imagen'], indent=4))\n",
        "print(ModelKeys['Imagen'])"
      ],
      "metadata": {
        "id": "mai7PkUvjWk_",
        "outputId": "4edf06a0-3989-4a78-8783-3108beacba99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mai7PkUvjWk_",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"dim\": 256,\n",
            "    \"text_embed_dim\": 544,\n",
            "    \"num_resnet_blocks\": 1,\n",
            "    \"cond_dim\": 512,\n",
            "    \"num_image_tokens\": null,\n",
            "    \"num_time_tokens\": null,\n",
            "    \"learned_sinu_pos_emb_dim\": null,\n",
            "    \"out_dim\": null,\n",
            "    \"dim_mults\": [\n",
            "        1,\n",
            "        2,\n",
            "        4,\n",
            "        8\n",
            "    ],\n",
            "    \"cond_images_channels\": 33,\n",
            "    \"channels\": 33,\n",
            "    \"channels_out\": 33,\n",
            "    \"attn_dim_head\": 64,\n",
            "    \"attn_heads\": 8,\n",
            "    \"ff_mult\": 2.0,\n",
            "    \"lowres_cond\": false,\n",
            "    \"layer_attns\": [\n",
            "        false,\n",
            "        true,\n",
            "        true,\n",
            "        false\n",
            "    ],\n",
            "    \"layer_attns_depth\": 1,\n",
            "    \"layer_attns_add_text_cond\": true,\n",
            "    \"attend_at_middle\": true,\n",
            "    \"layer_cross_attns\": [\n",
            "        false,\n",
            "        true,\n",
            "        true,\n",
            "        false\n",
            "    ],\n",
            "    \"use_linear_attn\": false,\n",
            "    \"use_linear_cross_attn\": false,\n",
            "    \"cond_on_text\": true,\n",
            "    \"max_text_len\": 128,\n",
            "    \"init_dim\": null,\n",
            "    \"resnet_groups\": 8,\n",
            "    \"init_conv_kernel_size\": 7,\n",
            "    \"init_cross_embed\": false,\n",
            "    \"init_cross_embed_kernel_sizes\": [\n",
            "        3,\n",
            "        7,\n",
            "        15\n",
            "    ],\n",
            "    \"cross_embed_downsample\": false,\n",
            "    \"cross_embed_downsample_kernel_sizes\": [\n",
            "        2,\n",
            "        4\n",
            "    ],\n",
            "    \"attn_pool_text\": true,\n",
            "    \"attn_pool_num_latents\": 32,\n",
            "    \"dropout\": 0.0,\n",
            "    \"memory_efficient\": false,\n",
            "    \"init_conv_to_final_conv_residual\": false,\n",
            "    \"use_global_context_attn\": true,\n",
            "    \"scale_skip_connection\": true,\n",
            "    \"final_resnet_block\": true,\n",
            "    \"final_conv_kernel_size\": 3,\n",
            "    \"cosine_sim_attn\": true,\n",
            "    \"self_cond\": false,\n",
            "    \"combine_upsample_fmaps\": true,\n",
            "    \"pixel_shuffle_upsample\": false,\n",
            "    \"beginning_and_final_conv_present\": null\n",
            "}\n",
            "{'timesteps': (96,), 'dim': 256, 'pred_dim': 33, 'loss_type': 0, 'elucidated': True, 'padding_idx': 0, 'cond_dim': None, 'text_embed_dim': None, 'input_tokens': None, 'sequence_embed': None, 'embed_dim_position': None, 'max_text_len': 128, 'cond_images_channels': 33, 'max_length': 128, 'device': device(type='cuda', index=0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# select some key values\n",
        "print(ModelKeys['UNet']['channels'])\n",
        "print(ModelKeys['UNet']['channels_out'])\n",
        "print(ModelKeys['UNet']['cond_images_channels'])\n",
        "print(ModelKeys['Imagen']['pred_dim'])\n",
        "print(ModelKeys['Imagen']['cond_images_channels'])\n",
        "# how large the model will be\n",
        "print()\n",
        "print(ModelKeys['UNet']['dim'])\n",
        "print(ModelKeys['Imagen']['dim'])"
      ],
      "metadata": {
        "id": "DFUubIDIkfto",
        "outputId": "3ed4f664-12b6-481a-d205-fe6f96dd4625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DFUubIDIkfto",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "33\n",
            "33\n",
            "33\n",
            "33\n",
            "\n",
            "256\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# setup the model\n",
        "# =====================================\n",
        "if CKeys['Problem_ID']==1 or CKeys['Problem_ID']==2:\n",
        "    pass\n",
        "#     # =================================================\n",
        "#     # Resideual level tasks: Model B\n",
        "#     # =================================================\n",
        "\n",
        "#     # working code: assemble the model archi\n",
        "#     print('--------------------------------------------')\n",
        "#     print('On OneD_Unet...')\n",
        "#     print('--------------------------------------------')\n",
        "#     working_unet = ModelPack.OneD_Unet(\n",
        "#         CKeys=CKeys,\n",
        "#         PKeys=ModelKeys['UNet'],\n",
        "#     ).to(device)\n",
        "\n",
        "#     print(\"Working unet part model: \")\n",
        "#     UtilityPack.params ( working_unet)\n",
        "\n",
        "#     print('--------------------------------------------')\n",
        "#     print('On whole model...')\n",
        "#     print('--------------------------------------------')\n",
        "#     # on the model part\n",
        "#     working_model = ModelPack.ProteinDesigner_B(\n",
        "#         working_unet,\n",
        "#         CKeys=CKeys,\n",
        "#         PKeys=ModelKeys['Imagen'],\n",
        "#     ). to(device)\n",
        "\n",
        "#     # measure\n",
        "#     print (\"Total working model: \")\n",
        "#     UtilityPack.params ( working_model)\n",
        "#     print (\"Recasted unet inside the tot one only: \")\n",
        "#     UtilityPack.params ( working_model.imagen.unets[0])\n",
        "\n",
        "#     print('--------------------------------------------')\n",
        "#     print('On trainer...')\n",
        "#     print('--------------------------------------------')\n",
        "#     working_trainer = TrainerPack.ImagenTrainer(\n",
        "#         working_model,\n",
        "#         CKeys=CKeys\n",
        "#     )\n",
        "\n",
        "elif CKeys['Problem_ID']==3 or CKeys['Problem_ID']==4:\n",
        "    pass\n",
        "#     # =================================================\n",
        "#     # Sequence level tasks: Model A\n",
        "#     # =================================================\n",
        "\n",
        "#     # working code: assemble the model archi\n",
        "#     print('--------------------------------------------')\n",
        "#     print('On OneD_Unet...')\n",
        "#     print('--------------------------------------------')\n",
        "#     working_unet = ModelPack.OneD_Unet(\n",
        "#         CKeys=CKeys,\n",
        "#         PKeys=ModelKeys['UNet'],\n",
        "#     ).to(device)\n",
        "\n",
        "#     print(\"Working unet part model: \")\n",
        "#     UtilityPack.params ( working_unet)\n",
        "\n",
        "#     print('--------------------------------------------')\n",
        "#     print('On whole model...')\n",
        "#     print('--------------------------------------------')\n",
        "#     # on the model part\n",
        "#     working_model = ModelPack.ProteinDesigner_A_II(\n",
        "#         working_unet,\n",
        "#         CKeys=CKeys,\n",
        "#         PKeys=ModelKeys['Imagen'],\n",
        "#     ). to(device)\n",
        "\n",
        "#     # measure\n",
        "#     print (\"Total working model: \")\n",
        "#     UtilityPack.params ( working_model)\n",
        "#     print (\"Recasted unet inside the tot one only: \")\n",
        "#     UtilityPack.params ( working_model.imagen.unets[0])\n",
        "\n",
        "#     print('--------------------------------------------')\n",
        "#     print('On trainer...')\n",
        "#     print('--------------------------------------------')\n",
        "#     working_trainer = TrainerPack.ImagenTrainer(\n",
        "#         working_model,\n",
        "#         CKeys=CKeys\n",
        "#     )\n",
        "\n",
        "elif CKeys['Problem_ID']==5 or CKeys['Problem_ID']==6 \\\n",
        "or   CKeys['Problem_ID']==11:\n",
        "    # =================================================\n",
        "    # Resideual level tasks: Model B\n",
        "    # =================================================\n",
        "\n",
        "    # working code: assemble the model archi\n",
        "    print('--------------------------------------------')\n",
        "    print('On OneD_Unet...')\n",
        "    print('--------------------------------------------')\n",
        "    working_unet = ModelPack.OneD_Unet(\n",
        "        CKeys=CKeys,\n",
        "        PKeys=ModelKeys['UNet'],\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Working unet part model: \")\n",
        "    UtilityPack.params ( working_unet)\n",
        "\n",
        "    print('--------------------------------------------')\n",
        "    print('On whole model...')\n",
        "    print('--------------------------------------------')\n",
        "    # on the model part\n",
        "    working_model = ModelPack.ProteinDesigner_B(\n",
        "        working_unet,\n",
        "        CKeys=CKeys,\n",
        "        PKeys=ModelKeys['Imagen'],\n",
        "    ). to(device)\n",
        "\n",
        "    # measure\n",
        "    print (\"Total working model: \")\n",
        "    UtilityPack.params ( working_model)\n",
        "    print (\"Recasted unet inside the tot one only: \")\n",
        "    UtilityPack.params ( working_model.imagen.unets[0])\n",
        "\n",
        "    print('--------------------------------------------')\n",
        "    print('On trainer...')\n",
        "    print('--------------------------------------------')\n",
        "    working_trainer = TrainerPack.ImagenTrainer(\n",
        "        working_model,\n",
        "        CKeys=CKeys\n",
        "    )\n",
        "\n",
        "elif CKeys['Problem_ID']==7 or CKeys['Problem_ID']==8:\n",
        "    # =================================================\n",
        "    # Sequence level tasks: Model A\n",
        "    # =================================================\n",
        "\n",
        "    # working code: assemble the model archi\n",
        "    print('--------------------------------------------')\n",
        "    print('On OneD_Unet...')\n",
        "    print('--------------------------------------------')\n",
        "    working_unet = ModelPack.OneD_Unet(\n",
        "        CKeys=CKeys,\n",
        "        PKeys=ModelKeys['UNet'],\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Working unet part model: \")\n",
        "    UtilityPack.params ( working_unet)\n",
        "\n",
        "    print('--------------------------------------------')\n",
        "    print('On whole model...')\n",
        "    print('--------------------------------------------')\n",
        "    # on the model part\n",
        "    working_model = ModelPack.ProteinDesigner_A_II(\n",
        "        working_unet,\n",
        "        CKeys=CKeys,\n",
        "        PKeys=ModelKeys['Imagen'],\n",
        "    ). to(device)\n",
        "\n",
        "    # measure\n",
        "    print (\"Total working model: \")\n",
        "    UtilityPack.params ( working_model)\n",
        "    print (\"Recasted unet inside the tot one only: \")\n",
        "    UtilityPack.params ( working_model.imagen.unets[0])\n",
        "\n",
        "    print('--------------------------------------------')\n",
        "    print('On trainer...')\n",
        "    print('--------------------------------------------')\n",
        "    working_trainer = TrainerPack.ImagenTrainer(\n",
        "        working_model,\n",
        "        CKeys=CKeys\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "RVVYlm6UkfxQ",
        "outputId": "2238596d-5fb0-47ec-aab0-933cd7d92ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RVVYlm6UkfxQ",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------\n",
            "On OneD_Unet...\n",
            "--------------------------------------------\n",
            "256 33\n",
            "Working unet part model: \n",
            "Total parameters:  264376242  trainable parameters:  264376242\n",
            "--------------------------------------------\n",
            "On whole model...\n",
            "--------------------------------------------\n",
            "Model B: Generative protein diffusion model, residue-based\n",
            "Using condition as the initial sequence\n",
            "Use conditioning image during training....\n",
            "Loss type:  0\n",
            "Channels in=33, channels out=33\n",
            "Test on cast_model_parameters...\n",
            "False\n",
            "False\n",
            "None\n",
            "33\n",
            "33\n",
            "256 33\n",
            "cpu\n",
            "Total working model: \n",
            "Total parameters:  262527570  trainable parameters:  262527570\n",
            "Recasted unet inside the tot one only: \n",
            "Total parameters:  262521266  trainable parameters:  262521266\n",
            "--------------------------------------------\n",
            "On trainer...\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "del working_unet\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8L2lbgmckf25"
      },
      "id": "8L2lbgmckf25",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Training part"
      ],
      "metadata": {
        "id": "rmt8n7ZOk47z"
      },
      "id": "rmt8n7ZOk47z"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if CKeys['Working_Mode']==1:\n",
        "    print(\"Training mode...\")\n",
        "\n",
        "    TrainKeys={}\n",
        "    if CKeys['Debug']==1:\n",
        "        TrainKeys['epochs']=4\n",
        "        TrainKeys['print_loss_every_this_epochs']=1\n",
        "        TrainKeys['sample_every_this_epochs']=1\n",
        "        TrainKeys['save_model_every_this_epochs']=2\n",
        "    else:\n",
        "        TrainKeys['epochs']=CKeys['epochs'] # 200\n",
        "        TrainKeys['print_loss_every_this_epochs']=CKeys['print_loss_every_this_epochs'] # 5\n",
        "        TrainKeys['sample_every_this_epochs']=CKeys['sample_every_this_epochs'] # 10\n",
        "        TrainKeys['save_model_every_this_epochs']=CKeys['save_model_every_this_epochs'] # 20"
      ],
      "metadata": {
        "id": "oP6SZ_omkf8k"
      },
      "id": "oP6SZ_omkf8k",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# one mini batch\n",
        "if CKeys['Debug']==1 and CKeys['Debug_DataPack']==1:\n",
        "    print(\"in a mini-batch:\")\n",
        "    print(\"input.dim:  \", this_item[0].shape)\n",
        "    # print(this_item[0][0,:]*torch.FloatTensor(DataKeys['Xnormfac']))\n",
        "    print(\"output.dim: \", this_item[1].shape)\n",
        "    print(DataKeys['Xnormfac'])"
      ],
      "metadata": {
        "id": "XVVo-PbCkf_a"
      },
      "id": "XVVo-PbCkf_a",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if CKeys['Working_Mode']==1:\n",
        "    # ==============================================================\n",
        "    # copy from the above when debug is done:\n",
        "    # ==============================================================\n",
        "    # a few files for restarts:\n",
        "    TRAINING_HIST_FILE   = ModelKeys['model_dir']+'Training_Hist.csv'\n",
        "    TRAINING_HIST_FILE_FULL = ModelKeys['model_dir']+'Training_Hist_Full.csv'\n",
        "    TRAINING_BREAK_POINT = ModelKeys['model_dir']+'Training_Info.txt'\n",
        "\n",
        "    if CKeys['Problem_ID']==1:\n",
        "        pass\n",
        "        # # de novo test\n",
        "        # test_condition_list = [\n",
        "        #     ['~~~HHHHHHHHHHHHHHH~~'],\n",
        "        #     ['~~EEESSTTS~SEEEEEEEEE~SBS~EEEEEE~~'],\n",
        "        # ]\n",
        "\n",
        "    elif CKeys['Problem_ID']==2:\n",
        "        pass\n",
        "\n",
        "        # test_condition_list = [\n",
        "        #     np.expand_dims(np.array(DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64))*DataKeys['Xnormfac']*0.33, axis=0),\n",
        "        #     np.expand_dims(np.array(DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64))*DataKeys['Xnormfac']*0.66, axis=0),\n",
        "        # ]\n",
        "\n",
        "    # de novo test\n",
        "    elif CKeys['Problem_ID']==3:\n",
        "        pass\n",
        "\n",
        "        # test_condition_list = [\n",
        "        #     [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
        "        #     [0, 0.7, 0.07, 0.1, 0.01, 0.02, 0.01, 0.11],\n",
        "        # ]\n",
        "\n",
        "    elif CKeys['Problem_ID']==4:\n",
        "        pass\n",
        "\n",
        "        # # here we used normalized (nFmax, nToughness)\n",
        "        # # the real values will be (nFmax, nToughness)*Xnormfac\n",
        "        # test_condition_list = [\n",
        "        #     [0.5, 0.5],\n",
        "        #     [0.2, 0.8],\n",
        "        # ]\n",
        "\n",
        "    # # #\n",
        "    # # test_condition_list = [\n",
        "    # #     np.expand_dims(np.array(this_item[0][10,:]*DataKeys['Xnormfac']), axis=0),\n",
        "    # #     np.expand_dims(np.array(this_item[0][20,:]*DataKeys['Xnormfac']), axis=0),\n",
        "    # # ]\n",
        "    # # protein_df['sample_NormPullGap_data'][pick_id_0]\n",
        "    # # DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64)\n",
        "    # # #\n",
        "    # test_condition_list = [\n",
        "    #     np.expand_dims(np.array(DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64))*DataKeys['Xnormfac']*0.33, axis=0),\n",
        "    #     np.expand_dims(np.array(DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64))*DataKeys['Xnormfac']*0.66, axis=0),\n",
        "    # ]\n",
        "    elif CKeys['Problem_ID']==5:\n",
        "        pass\n",
        "\n",
        "        # # de novo test\n",
        "        # test_condition_list = [\n",
        "        #     ['~~~HHHHHHHHHHHHHHH~~'],\n",
        "        #     ['~~EEESSTTS~SEEEEEEEEE~SBS~EEEEEE~~'],\n",
        "        # ]\n",
        "\n",
        "    #\n",
        "    elif CKeys['Problem_ID']==6 or CKeys['Problem_ID']==11:\n",
        "        # for ESM model: 0+content+00\n",
        "        # test_condition_list = [\n",
        "        #     np.expand_dims(np.array([0]+DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64-1))*DataKeys['Xnormfac']*0.33, axis=0),\n",
        "        #     np.expand_dims(np.array([0]+DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64-1))*DataKeys['Xnormfac']*0.66, axis=0),\n",
        "        # ]\n",
        "        #\n",
        "        # test_condition_list = [\n",
        "        #     np.insert(np.array(DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64-1))*DataKeys['Xnormfac']*0.33, 0, 0.),\n",
        "        #     np.insert(np.array(DataSetPack.pad_a_np_arr(protein_df['sample_NormPullGap_data'][pick_id_0],0,64-1))*DataKeys['Xnormfac']*0.66, 0, 0.),\n",
        "        # ]\n",
        "        #\n",
        "        test_0 = DataSetPack.pad_a_np_arr(\n",
        "            protein_df['sample_FORCE_data'][pick_id_0],\n",
        "            0.,\n",
        "            DataKeys['max_AA_seq_len']\n",
        "        )\n",
        "        test_1 = DataSetPack.pad_a_np_arr_esm(\n",
        "            protein_df['sample_FORCE_data'][pick_id_1],\n",
        "            0.,\n",
        "            DataKeys['max_AA_seq_len']\n",
        "        )\n",
        "        # test_condition_list = [\n",
        "        #     test_0*0.33,\n",
        "        #     test_0*0.66,\n",
        "        #     test_1*0.33,\n",
        "        #     test_1*0.66,\n",
        "        # ]\n",
        "        test_condition_list = [\n",
        "            test_0*0.66,\n",
        "            test_1*1.66,\n",
        "        ]\n",
        "\n",
        "\n",
        "    #\n",
        "    # de novo test\n",
        "    elif CKeys['Problem_ID']==7:\n",
        "        pass\n",
        "\n",
        "        # test_condition_list = [\n",
        "        #     [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
        "        #     [0, 0.7, 0.07, 0.1, 0.01, 0.02, 0.01, 0.11],\n",
        "        # ]\n",
        "\n",
        "    elif CKeys['Problem_ID']==8:\n",
        "\n",
        "        test_condition_list = [\n",
        "            [0.2, 0.8]*DataKeys['Xnormfac'],\n",
        "            [0.8, 0.2]*DataKeys['Xnormfac'],\n",
        "        ]\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No de novo condition is found for the problem...\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OYGtJTD7kgB-"
      },
      "id": "OYGtJTD7kgB-",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Check tokenizer_X:', tokenizer_X)\n",
        "print('tokenizer_y:', tokenizer_y)"
      ],
      "metadata": {
        "id": "B_ZVY_uSlFri",
        "outputId": "bd21c8bd-0076-4f3c-ef88-468189b233ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B_ZVY_uSlFri",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check tokenizer_X: None\n",
            "tokenizer_y: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skip training part"
      ],
      "metadata": {
        "id": "XGMWgbfSlFus"
      },
      "id": "XGMWgbfSlFus",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5. Testing loop"
      ],
      "metadata": {
        "id": "t52CdrtKlv6z"
      },
      "id": "t52CdrtKlv6z"
    },
    {
      "cell_type": "code",
      "source": [
        "if CKeys['Working_Mode']==2:\n",
        "    print(\"This is testing model...\")\n",
        "    # setup test key\n",
        "    TestKeys={}\n",
        "    TestKeys['Sample_On_TestSet']=False # True # True # False\n",
        "    TestKeys['Sample_On_DeNovo']=True\n",
        "    # create a test dir\n",
        "    # TestKeys['test_dir']=PKeys['prefix']+'2_test/'\n",
        "    # TestKeys['test_dir']=PKeys['prefix']+'3_test_common_denovo/'\n",
        "    # add for colab\n",
        "    TestKeys['test_dir']=PKeys['prefix']+'4_test_individual_cases/'\n",
        "    UtilityPack.create_path(TestKeys['test_dir'])"
      ],
      "metadata": {
        "id": "k65qjerclFxn",
        "outputId": "e6239174-61da-4681-cf2a-40314d756c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k65qjerclFxn",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is testing model...\n",
            "Creating the given path...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(PKeys['prefix'])"
      ],
      "metadata": {
        "id": "nMDVeScrlF0d",
        "outputId": "08c50a1e-da48-4d86-a979-c100a96fd54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nMDVeScrlF0d",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/working_results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if CKeys['Working_Mode']==2:\n",
        "    # skip for colab inferring\n",
        "    pass\n",
        "    # # looking back at the training\n",
        "    # print(\"Check the training history...\")\n",
        "    # #\n",
        "    # # training history\n",
        "    # TRAINING_HIST_FILE   = ModelKeys['model_dir']+'Training_Hist.csv'\n",
        "    # TRAINING_HIST_FILE_FULL = ModelKeys['model_dir']+'Training_Hist_Full.csv'\n",
        "    # TRAINING_BREAK_POINT = ModelKeys['model_dir']+'Training_Info.txt'\n",
        "\n",
        "    # # pick up the leftover info\n",
        "    # # train_rec = pd.read_csv(TRAINING_HIST_FILE)\n",
        "    # train_rec = pd.read_csv(TRAINING_HIST_FILE_FULL)\n",
        "    # #\n",
        "    # id_best_loss = np.argmin(train_rec['norm_loss'])\n",
        "    # best_epoch = train_rec['epoch'][id_best_loss]\n",
        "    # # print(best_epoch)\n",
        "    # print(f\"Best epoch: {best_epoch}; LOSS: {train_rec['norm_loss'][id_best_loss]}\", )\n",
        "\n",
        "    # fig = plt.figure()\n",
        "    # plt.plot(\n",
        "    #     train_rec['epoch'],\n",
        "    #     train_rec['norm_loss'],\n",
        "    #     label='Loss',\n",
        "    # )\n",
        "    # plt.legend()\n",
        "    # outname=TestKeys['test_dir']+\"0_Training_Hist.jpg\"\n",
        "    # if CKeys['SlientRun']==1:\n",
        "    #     plt.savefig(outname, dpi=200)\n",
        "    # else:\n",
        "    #     pass\n",
        "    # plt.show()\n",
        "    # plt.close(fig)\n",
        "\n",
        "    # #\n",
        "    # # pick up the breaking point of last time\n",
        "    # breaking_rec = pd.read_csv(TRAINING_BREAK_POINT)\n",
        "    # last_epoch = breaking_rec['epoch'][0]\n",
        "    # last_step  = breaking_rec['steps'][0]\n",
        "    # # print(\"Last epoch: \", breaking_rec['epoch'][0])\n",
        "    # print(f\"Last epoch: {last_epoch}; LOSS: {breaking_rec['norm_loss'][0]}\", )\n",
        "    #\n",
        "    # last_epoch = 4000\n",
        "    # last_step = 92000\n",
        "\n"
      ],
      "metadata": {
        "id": "17Ooj12FlF3L"
      },
      "id": "17Ooj12FlF3L",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CKeys['Working_Mode']==2:\n",
        "    print(\"Load back the model at the LAST epoch...\")\n",
        "    # load back the LAST model:\n",
        "    # model_last_fname=f\"{ModelKeys['model_dir_model']}trainer_save-model-epoch_{last_epoch}.pt\"\n",
        "    model_last_fname=f\"{this_working_path}1_model_SS/trainer_save-model_pLDM.pt\"\n",
        "    working_trainer.load(model_last_fname)\n",
        "    # TBA: load back the Best model AVAILABLE"
      ],
      "metadata": {
        "id": "5nXuPCpLmP0w"
      },
      "id": "5nXuPCpLmP0w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ModelKeys['model_dir_model'])\n",
        "# print(last_epoch)"
      ],
      "metadata": {
        "id": "W3W0QWRjmP3z",
        "outputId": "59d42260-64d0-4fd2-e737-4ff5dd91fb39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W3W0QWRjmP3z",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/1_model_SS/1_store_model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1AVpAhUmP7M"
      },
      "id": "u1AVpAhUmP7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-M-3xDrlF59"
      },
      "id": "5-M-3xDrlF59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PKeys['prefix']=this_working_path\n",
        "PKeys['pk_data_pack']=this_working_path+'data_pack.pickle'\n",
        "PKeys['pk_model_pack']=this_working_path+'model_pack.pickle'"
      ],
      "metadata": {
        "id": "zpz56o4XhvhE",
        "outputId": "c0e031b9-ef11-4a68-9fdc-52e8b8ef25d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zpz56o4XhvhE",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prefix': '/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/',\n",
              " 'pk_data_pack': '/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/data_pack.pickle',\n",
              " 'pk_model_pack': '/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/model_pack.pickle'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DataKeys['data_dir'])"
      ],
      "metadata": {
        "id": "Mo8scfhnRZLe",
        "outputId": "6b49460c-ac3d-4355-d2e3-d873fad7d690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Mo8scfhnRZLe",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/gridsan/bni/16_WG_git_sort_pdb_database_0/11_pLMProb_Diff_SMD_ModelB_embed_640/0_Training/0_dataprocess_MD/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxFqwGM5RZOS"
      },
      "id": "bxFqwGM5RZOS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PKeys['pk_data_pack'])"
      ],
      "metadata": {
        "id": "MPk7OefFRZRA",
        "outputId": "f2d3c2d9-d0a4-40d8-d5d1-4e0d0fab5cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MPk7OefFRZRA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Trained_model/data_pack.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(PKeys['pk_data_pack'])\n"
      ],
      "metadata": {
        "id": "AQ2dE_8vARaK",
        "outputId": "e236055d-c8f7-4371-f92b-61220ccc3bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AQ2dE_8vARaK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Trained_model/data_pack.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fk5K2mKt_d7_"
      },
      "id": "fk5K2mKt_d7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ModelPack"
      ],
      "metadata": {
        "id": "YX9tFYEZ8aMb"
      },
      "id": "YX9tFYEZ8aMb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f755b5-7c61-48a9-878d-ab01cab4baf6",
      "metadata": {
        "id": "78f755b5-7c61-48a9-878d-ab01cab4baf6",
        "outputId": "2950dd61-fa2e-4079-ed0e-0ccd4c605c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ],
      "source": [
        "print('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a1e936-2879-4ac6-add6-718482b9ca38",
      "metadata": {
        "id": "76a1e936-2879-4ac6-add6-718482b9ca38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}